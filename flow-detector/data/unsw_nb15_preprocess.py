import pandas as pd
import json
import os
import numpy as np
import hashlib
import time
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from collections import defaultdict
from sklearn.preprocessing import StandardScaler, LabelEncoder
import torch
from torch.utils.data import DataLoader, TensorDataset
import logging
import tempfile
import shutil
from pathlib import Path
import argparse
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class FlowInfo:
    """ç½‘ç»œæµä¿¡æ¯æ•°æ®ç±»"""

    flow_id: str
    src_ip: str
    dst_ip: str
    src_port: int
    dst_port: int
    protocol: str
    start_time: float
    end_time: float
    packets: List[Dict]

    def duration(self) -> float:
        return self.end_time - self.start_time

    def packet_count(self) -> int:
        return len(self.packets)

    def total_bytes(self) -> int:
        return sum(pkt.get("length", 0) for pkt in self.packets)


class AdvancedPcapProcessor:
    """é«˜çº§PCAPå¤„ç†å™¨ - æ”¯æŒå®Œæ•´çš„æµé‡åˆ†æ"""

    def __init__(self):
        self.flows = {}
        self.flow_timeout = 60  # æµè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    def read_pcap_advanced(
        self, file_path: str, max_packets: int = None
    ) -> pd.DataFrame:
        """
        é«˜çº§PCAPè¯»å–å’Œç‰¹å¾æå–

        Args:
            file_path: PCAPæ–‡ä»¶è·¯å¾„
            max_packets: æœ€å¤§è¯»å–åŒ…æ•°é‡ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰

        Returns:
            åŒ…å«ä¸°å¯Œç‰¹å¾çš„DataFrame
        """
        try:
            from scapy.all import rdpcap, IP, TCP, UDP, ICMP
        except ImportError:
            raise ImportError(
                "scapy is required for advanced PCAP processing. "
                "Install with: pip install scapy"
            )

        logger.info(f"å¼€å§‹è¯»å–PCAPæ–‡ä»¶: {file_path}")
        packets = rdpcap(file_path)

        if max_packets is not None and len(packets) > max_packets:
            logger.warning(f"åŒ…æ•°é‡è¿‡å¤š ({len(packets)})ï¼Œä»…å¤„ç†å‰ {max_packets} ä¸ªåŒ…")
            packets = packets[:max_packets]
        else:
            logger.info(f"è¯»å–å®Œæˆï¼Œå…± {len(packets)} ä¸ªæ•°æ®åŒ…ï¼Œå°†å…¨éƒ¨å¤„ç†")

        # ç¬¬ä¸€æ­¥ï¼šæµé‡åˆ†å‰²ä¸è§£æ
        flows = self._segment_traffic(packets)

        # ç¬¬äºŒæ­¥ï¼šç‰¹å¾æå–
        features_df = self._extract_flow_features(flows)

        # ç¬¬ä¸‰æ­¥ï¼šæ•°æ®æ¸…æ´—
        cleaned_df = self._clean_data(features_df)

        logger.info(f"å¤„ç†å®Œæˆ: {len(cleaned_df)} ä¸ªæµ")
        return cleaned_df

    def _segment_traffic(self, packets) -> List[FlowInfo]:
        """æµé‡åˆ†å‰² - å°†æ•°æ®åŒ…æŒ‰è¿æ¥åˆ†ç»„"""
        from scapy.all import IP, TCP, UDP

        flows = defaultdict(list)
        flow_times = {}

        for pkt in packets:
            if not IP in pkt:
                continue

            # æå–åŸºæœ¬ä¿¡æ¯
            ip_layer = pkt[IP]
            src_ip = ip_layer.src
            dst_ip = ip_layer.dst
            protocol = ip_layer.proto
            timestamp = float(pkt.time)

            # æå–ç«¯å£ä¿¡æ¯
            src_port = dst_port = 0
            if TCP in pkt:
                src_port = pkt[TCP].sport
                dst_port = pkt[TCP].dport
                protocol_name = "tcp"
            elif UDP in pkt:
                src_port = pkt[UDP].sport
                dst_port = pkt[UDP].dport
                protocol_name = "udp"
            else:
                protocol_name = "other"

            # ç”ŸæˆæµIDï¼ˆåŒå‘æµåˆå¹¶ï¼‰
            flow_id = self._generate_flow_id(
                src_ip, dst_ip, src_port, dst_port, protocol_name
            )

            # æ„é€ åŒ…ä¿¡æ¯
            packet_info = {
                "timestamp": timestamp,
                "length": len(pkt),
                "src_ip": src_ip,
                "dst_ip": dst_ip,
                "src_port": src_port,
                "dst_port": dst_port,
                "protocol": protocol_name,
                "flags": self._extract_tcp_flags(pkt) if TCP in pkt else "",
                "payload_size": len(pkt.payload) if hasattr(pkt, "payload") else 0,
            }

            flows[flow_id].append(packet_info)

            # æ›´æ–°æµæ—¶é—´èŒƒå›´
            if flow_id not in flow_times:
                flow_times[flow_id] = {"start": timestamp, "end": timestamp}
            else:
                flow_times[flow_id]["start"] = min(
                    flow_times[flow_id]["start"], timestamp
                )
                flow_times[flow_id]["end"] = max(flow_times[flow_id]["end"], timestamp)

        # è½¬æ¢ä¸ºFlowInfoå¯¹è±¡
        flow_objects = []
        for flow_id, packets in flows.items():
            if not packets:
                continue

            first_pkt = packets[0]
            time_info = flow_times[flow_id]

            flow_obj = FlowInfo(
                flow_id=flow_id,
                src_ip=first_pkt["src_ip"],
                dst_ip=first_pkt["dst_ip"],
                src_port=first_pkt["src_port"],
                dst_port=first_pkt["dst_port"],
                protocol=first_pkt["protocol"],
                start_time=time_info["start"],
                end_time=time_info["end"],
                packets=packets,
            )
            flow_objects.append(flow_obj)

        logger.info(f"åˆ†å‰²å‡º {len(flow_objects)} ä¸ªç½‘ç»œæµ")
        return flow_objects

    def _generate_flow_id(
        self, src_ip: str, dst_ip: str, src_port: int, dst_port: int, protocol: str
    ) -> str:
        """ç”ŸæˆåŒå‘æµID"""
        # ç¡®ä¿åŒå‘æµä½¿ç”¨ç›¸åŒID
        if src_ip < dst_ip or (src_ip == dst_ip and src_port < dst_port):
            flow_tuple = (src_ip, dst_ip, src_port, dst_port, protocol)
        else:
            flow_tuple = (dst_ip, src_ip, dst_port, src_port, protocol)

        flow_string = f"{flow_tuple[0]}:{flow_tuple[2]}-{flow_tuple[1]}:{flow_tuple[3]}:{flow_tuple[4]}"
        return hashlib.md5(flow_string.encode()).hexdigest()[:12]

    def _extract_tcp_flags(self, pkt) -> str:
        """æå–TCPæ ‡å¿—ä½"""
        try:
            from scapy.all import TCP

            if TCP in pkt:
                flags = []
                tcp_layer = pkt[TCP]
                if tcp_layer.flags & 0x01:
                    flags.append("FIN")
                if tcp_layer.flags & 0x02:
                    flags.append("SYN")
                if tcp_layer.flags & 0x04:
                    flags.append("RST")
                if tcp_layer.flags & 0x08:
                    flags.append("PSH")
                if tcp_layer.flags & 0x10:
                    flags.append("ACK")
                if tcp_layer.flags & 0x20:
                    flags.append("URG")
                return ",".join(flags)
        except:
            pass
        return ""

    def _extract_flow_features(self, flows: List[FlowInfo]) -> pd.DataFrame:
        """ç‰¹å¾æå–å·¥ç¨‹ - ä»æµä¸­æå–UNSW-NB15å…¼å®¹ç‰¹å¾"""
        features_list = []

        for flow in flows:
            try:
                # åŸºç¡€æµç‰¹å¾
                duration = flow.duration()
                packet_count = flow.packet_count()
                total_bytes = flow.total_bytes()

                # åˆ†æ–¹å‘ç»Ÿè®¡
                src_packets, dst_packets = self._count_directional_packets(flow)
                src_bytes, dst_bytes = self._count_directional_bytes(flow)

                # æ—¶é—´ç‰¹å¾
                packet_rate = packet_count / max(duration, 0.001)
                byte_rate = total_bytes / max(duration, 0.001)

                # åŒ…å¤§å°ç»Ÿè®¡
                packet_sizes = [pkt["length"] for pkt in flow.packets]
                avg_packet_size = np.mean(packet_sizes) if packet_sizes else 0
                std_packet_size = np.std(packet_sizes) if len(packet_sizes) > 1 else 0

                # æ—¶é—´é—´éš”ç»Ÿè®¡
                if len(flow.packets) > 1:
                    intervals = []
                    for i in range(1, len(flow.packets)):
                        interval = (
                            flow.packets[i]["timestamp"]
                            - flow.packets[i - 1]["timestamp"]
                        )
                        intervals.append(interval)
                    avg_interval = np.mean(intervals)
                    std_interval = np.std(intervals)
                else:
                    avg_interval = std_interval = 0

                # åè®®ç‰¹å¼‚æ€§ç‰¹å¾
                protocol_features = self._extract_protocol_specific_features(flow)

                # æ„é€ ç‰¹å¾å­—å…¸ï¼ˆå…¼å®¹UNSW-NB15æ ¼å¼ï¼‰
                features = {
                    # åŸºç¡€ç‰¹å¾
                    "id": len(features_list) + 1,
                    "dur": duration,
                    "proto": flow.protocol,
                    "service": self._identify_service(flow),
                    "state": self._determine_connection_state(flow),
                    # åŒ…å’Œå­—èŠ‚è®¡æ•°
                    "spkts": src_packets,
                    "dpkts": dst_packets,
                    "sbytes": src_bytes,
                    "dbytes": dst_bytes,
                    # é€Ÿç‡ç‰¹å¾
                    "rate": packet_rate,
                    "sload": src_bytes / max(duration, 0.001),
                    "dload": dst_bytes / max(duration, 0.001),
                    # åŒ…å¤§å°ç‰¹å¾
                    "smean": avg_packet_size,
                    "dmean": avg_packet_size,
                    # æ—¶é—´ç‰¹å¾
                    "sinpkt": avg_interval * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                    "dinpkt": avg_interval * 1000,
                    "sjit": std_interval * 1000,
                    "djit": std_interval * 1000,
                    # åè®®ç‰¹å®šç‰¹å¾
                    **protocol_features,
                    # é»˜è®¤å€¼ï¼ˆä¸UNSW-NB15å…¼å®¹ï¼‰
                    "sttl": 64,
                    "dttl": 64,
                    "sloss": 0,
                    "dloss": 0,
                    "swin": 8192,
                    "dwin": 8192,
                    "stcpb": 0,
                    "dtcpb": 0,
                    "tcprtt": 0,
                    "synack": 0,
                    "ackdat": 0,
                    "trans_depth": 0,
                    "response_body_len": 0,
                    # è®¡æ•°ç‰¹å¾
                    "ct_srv_src": 1,
                    "ct_state_ttl": 1,
                    "ct_dst_ltm": 1,
                    "ct_src_dport_ltm": 1,
                    "ct_dst_sport_ltm": 1,
                    "ct_dst_src_ltm": 1,
                    "ct_src_ltm": 1,
                    "ct_srv_dst": 1,
                    # å†…å®¹ç‰¹å¾
                    "is_ftp_login": 1 if flow.dst_port == 21 else 0,
                    "ct_ftp_cmd": 0,
                    "ct_flw_http_mthd": 1 if flow.dst_port in [80, 443, 8080] else 0,
                    "is_sm_ips_ports": 1 if flow.src_ip == flow.dst_ip else 0,
                    # é»˜è®¤æ ‡ç­¾ï¼ˆPCAPæ–‡ä»¶é»˜è®¤ä¸ºæ­£å¸¸æµé‡ï¼‰
                    "attack_cat": "Normal",
                    "label": "Normal",
                }

                features_list.append(features)

            except Exception as e:
                logger.warning(f"ç‰¹å¾æå–å¤±è´¥ (æµ {flow.flow_id}): {e}")
                continue

        return pd.DataFrame(features_list)

    def _count_directional_packets(self, flow: FlowInfo) -> Tuple[int, int]:
        """ç»Ÿè®¡åŒå‘åŒ…æ•°é‡"""
        src_count = dst_count = 0
        for pkt in flow.packets:
            if pkt["src_ip"] == flow.src_ip:
                src_count += 1
            else:
                dst_count += 1
        return src_count, dst_count

    def _count_directional_bytes(self, flow: FlowInfo) -> Tuple[int, int]:
        """ç»Ÿè®¡åŒå‘å­—èŠ‚æ•°"""
        src_bytes = dst_bytes = 0
        for pkt in flow.packets:
            if pkt["src_ip"] == flow.src_ip:
                src_bytes += pkt["length"]
            else:
                dst_bytes += pkt["length"]
        return src_bytes, dst_bytes

    def _extract_protocol_specific_features(self, flow: FlowInfo) -> Dict[str, Any]:
        """æå–åè®®ç‰¹å¼‚æ€§ç‰¹å¾"""
        features = {}

        if flow.protocol == "tcp":
            # TCPç‰¹å¼‚æ€§ç‰¹å¾
            syn_count = sum(1 for pkt in flow.packets if "SYN" in pkt.get("flags", ""))
            fin_count = sum(1 for pkt in flow.packets if "FIN" in pkt.get("flags", ""))
            rst_count = sum(1 for pkt in flow.packets if "RST" in pkt.get("flags", ""))

            features.update(
                {
                    "tcp_syn_count": syn_count,
                    "tcp_fin_count": fin_count,
                    "tcp_rst_count": rst_count,
                }
            )

        elif flow.protocol == "udp":
            # UDPç‰¹å¼‚æ€§ç‰¹å¾
            features.update(
                {
                    "udp_packet_rate": len(flow.packets) / max(flow.duration(), 0.001),
                }
            )

        return features

    def _identify_service(self, flow: FlowInfo) -> str:
        """è¯†åˆ«ç½‘ç»œæœåŠ¡ç±»å‹"""
        port_services = {
            21: "ftp",
            22: "ssh",
            23: "telnet",
            25: "smtp",
            53: "dns",
            80: "http",
            110: "pop3",
            143: "imap",
            443: "https",
            993: "imaps",
            995: "pop3s",
            8080: "http-alt",
            8443: "https-alt",
        }

        # æ£€æŸ¥ç›®æ ‡ç«¯å£
        if flow.dst_port in port_services:
            return port_services[flow.dst_port]

        # æ£€æŸ¥æºç«¯å£ï¼ˆåå‘è¿æ¥ï¼‰
        if flow.src_port in port_services:
            return port_services[flow.src_port]

        return "-"  # UNSW-NB15ä¸­çš„é»˜è®¤å€¼

    def _determine_connection_state(self, flow: FlowInfo) -> str:
        """ç¡®å®šè¿æ¥çŠ¶æ€"""
        if flow.protocol != "tcp":
            return "CON"  # UDPç­‰æ— è¿æ¥åè®®

        # åˆ†æTCPæ ‡å¿—ä½ç¡®å®šçŠ¶æ€
        has_syn = any("SYN" in pkt.get("flags", "") for pkt in flow.packets)
        has_fin = any("FIN" in pkt.get("flags", "") for pkt in flow.packets)
        has_rst = any("RST" in pkt.get("flags", "") for pkt in flow.packets)

        if has_rst:
            return "RST"
        elif has_fin:
            return "FIN"
        elif has_syn:
            return "CON"
        else:
            return "INT"  # ä¸­é—´çŠ¶æ€

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—å’Œå»å™ª"""
        logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")

        # è®°å½•åŸå§‹æ•°æ®é‡
        original_count = len(df)

        # 1. ç§»é™¤ç©ºå€¼è¿‡å¤šçš„è¡Œ
        df = df.dropna(thresh=len(df.columns) * 0.5)

        # 2. ç§»é™¤å¼‚å¸¸çŸ­çš„æµï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰
        df = df[df["dur"] >= 0.0]  # æŒç»­æ—¶é—´ä¸èƒ½ä¸ºè´Ÿ
        df = df[df["spkts"] + df["dpkts"] >= 1]  # è‡³å°‘è¦æœ‰1ä¸ªåŒ…

        # 3. ç§»é™¤å¼‚å¸¸å¤§çš„å€¼ï¼ˆå¯èƒ½æ˜¯é”™è¯¯æ•°æ®ï¼‰
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col in ["sbytes", "dbytes"]:
                # å­—èŠ‚æ•°ä¸Šé™ï¼ˆ10GBï¼‰
                df = df[df[col] <= 10 * 1024 * 1024 * 1024]
            elif col in ["spkts", "dpkts"]:
                # åŒ…æ•°ä¸Šé™ï¼ˆ100ä¸‡ï¼‰
                df = df[df[col] <= 1000000]
            elif col == "dur":
                # æŒç»­æ—¶é—´ä¸Šé™ï¼ˆ1å°æ—¶ï¼‰
                df = df[df[col] <= 3600]

        # 4. å¡«å……ç¼ºå¤±å€¼
        for col in numeric_columns:
            df[col] = df[col].fillna(0)

        # 5. ä¿®å¤å­—ç¬¦ä¸²åˆ—
        string_columns = df.select_dtypes(include=["object"]).columns
        for col in string_columns:
            df[col] = df[col].fillna("-")

        # 6. é‡æ–°ç´¢å¼•
        df = df.reset_index(drop=True)
        df["id"] = range(1, len(df) + 1)

        cleaned_count = len(df)
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {original_count} â†’ {cleaned_count} æ¡è®°å½•")

        return df


# ä¿æŒå‘åå…¼å®¹çš„ç®€å•æ¥å£
def read_pcap(file_path, max_packets=None):
    """å‘åå…¼å®¹çš„ç®€å•PCAPè¯»å–æ¥å£"""
    processor = AdvancedPcapProcessor()
    return processor.read_pcap_advanced(file_path, max_packets)


def load_file(file_path):
    """åŠ è½½å„ç§æ ¼å¼çš„æ•°æ®æ–‡ä»¶"""
    ext = os.path.splitext(file_path)[-1].lower()

    if ext == ".csv":
        return pd.read_csv(file_path)
    elif ext == ".tsv":
        return pd.read_csv(file_path, sep="\t")
    elif ext == ".json":
        with open(file_path, "r") as f:
            return pd.DataFrame(json.load(f))
    elif ext in [".parquet", ".feather"]:
        return pd.read_parquet(file_path)
    elif ext in [".pcap", ".pcapng"]:
        # ä½¿ç”¨é«˜çº§PCAPå¤„ç†å™¨
        processor = AdvancedPcapProcessor()
        return processor.read_pcap_advanced(file_path)
    elif ext in [".log", ".txt"]:
        return pd.read_csv(file_path, sep=r"\s+")
    else:
        raise ValueError(f"Unsupported file type: {ext}")


def preprocess_df(df, drop_service=True):
    """é¢„å¤„ç†DataFrameæ•°æ®"""
    df = df.copy()
    df = df.sample(frac=1).reset_index(drop=True)

    if "attack_cat" in df.columns:
        df["label"] = df["attack_cat"].apply(
            lambda x: 0 if str(x).lower() == "normal" else 1
        )
    elif "label" not in df.columns:
        df["label"] = 0  # pcap é»˜è®¤å…¨éƒ¨æ˜¯æ­£å¸¸æµé‡

    drop_cols = ["id", "attack_cat", "label"]
    if drop_service and "service" in df.columns:
        drop_cols.append("service")

    columns_to_drop = [col for col in drop_cols if col in df.columns]
    X = df.drop(columns=columns_to_drop, errors="ignore")
    y = df["label"]

    for col in X.select_dtypes(include="object").columns:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str))

    return X, y


def load_train_test(train_path, test_path, batch_size=256, drop_service=True):
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
    train_df = load_file(train_path)
    test_df = load_file(test_path)

    X_train, y_train = preprocess_df(train_df, drop_service)
    X_test, y_test = preprocess_df(test_df, drop_service)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test[X_train.columns])

    train_dataset = TensorDataset(
        torch.tensor(X_train_scaled, dtype=torch.float32),
        torch.tensor(y_train.values, dtype=torch.long),
    )
    test_dataset = TensorDataset(
        torch.tensor(X_test_scaled, dtype=torch.float32),
        torch.tensor(y_test.values, dtype=torch.long),
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, X_train.shape[1], scaler


class FrontendPcapHandler:
    """å‰ç«¯PCAPæ–‡ä»¶å¤„ç†æ¥å£"""

    def __init__(self, upload_dir: str = "uploads", output_dir: str = "processed"):
        """
        åˆå§‹åŒ–å‰ç«¯å¤„ç†å™¨

        Args:
            upload_dir: ä¸Šä¼ æ–‡ä»¶ç›®å½•
            output_dir: å¤„ç†ç»“æœè¾“å‡ºç›®å½•
        """
        self.upload_dir = upload_dir
        self.output_dir = output_dir
        self.processor = AdvancedPcapProcessor()
        self.converter = PcapToCSVConverter(output_dir)

        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(upload_dir, exist_ok=True)
        os.makedirs(output_dir, exist_ok=True)

    def handle_uploaded_pcap(self, file_path: str, filename: str) -> Dict[str, Any]:
        """
        å¤„ç†å‰ç«¯ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆPCAPæˆ–CSVï¼‰

        Args:
            file_path: ä¸Šä¼ æ–‡ä»¶çš„ä¸´æ—¶è·¯å¾„
            filename: åŸå§‹æ–‡ä»¶å

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶: {filename}")

            # è·å–æ–‡ä»¶æ‰©å±•å
            ext = Path(filename).suffix.lower()

            if ext == ".csv":
                # å¤„ç†CSVæ–‡ä»¶
                return self._handle_csv_file(file_path, filename)
            elif ext in [".pcap", ".pcapng"]:
                # å¤„ç†PCAPæ–‡ä»¶
                return self._handle_pcap_file(file_path, filename)
            else:
                return {
                    "success": False,
                    "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ .pcapã€.pcapngæˆ–.csvæ–‡ä»¶",
                    "code": "INVALID_FORMAT",
                }

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "code": "PROCESSING_ERROR"}

    def _handle_csv_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """
        å¤„ç†CSVæ–‡ä»¶

        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            filename: åŸå§‹æ–‡ä»¶å

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # è¯»å–CSVæ–‡ä»¶
            import pandas as pd

            df = pd.read_csv(file_path)

            if len(df) == 0:
                return {
                    "success": False,
                    "error": "CSVæ–‡ä»¶ä¸ºç©º",
                    "code": "EMPTY_FILE",
                }

            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_filename = self._generate_safe_filename(filename)
            target_path = os.path.join(self.upload_dir, safe_filename)

            # å¤åˆ¶æ–‡ä»¶åˆ°ä¸Šä¼ ç›®å½•
            shutil.copy2(file_path, target_path)

            # CSVæ–‡ä»¶å¯ä»¥ç›´æ¥ç”¨äºåˆ†æï¼Œå¤åˆ¶åˆ°è¾“å‡ºç›®å½•
            csv_filename = f"{Path(safe_filename).stem}_processed.csv"
            csv_path = os.path.join(self.output_dir, csv_filename)
            df.to_csv(csv_path, index=False)

            # ç”Ÿæˆå¤„ç†æ‘˜è¦
            summary = self._generate_processing_summary(df, safe_filename)

            logger.info(f"CSVæ–‡ä»¶å¤„ç†å®Œæˆ: {len(df)} è¡Œæ•°æ®")

            return {
                "success": True,
                "processed_flows": len(df),
                "csv_file": csv_path,
                "csv_filename": csv_filename,
                "summary": summary,
                "original_filename": filename,
                "processing_time": 0.1,  # CSVå¤„ç†å¾ˆå¿«
            }

        except Exception as e:
            logger.error(f"å¤„ç†CSVæ–‡ä»¶å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"CSVæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}",
                "code": "CSV_PROCESSING_ERROR",
            }

    def _handle_pcap_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """
        å¤„ç†PCAPæ–‡ä»¶

        Args:
            file_path: PCAPæ–‡ä»¶è·¯å¾„
            filename: åŸå§‹æ–‡ä»¶å

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # 1. éªŒè¯æ–‡ä»¶æ ¼å¼
            if not self._validate_pcap_file(file_path):
                return {
                    "success": False,
                    "error": "æ— æ•ˆçš„PCAPæ–‡ä»¶æ ¼å¼",
                    "code": "INVALID_FORMAT",
                }

            # 2. ç§»åŠ¨æ–‡ä»¶åˆ°å¤„ç†ç›®å½•
            safe_filename = self._generate_safe_filename(filename)
            target_path = os.path.join(self.upload_dir, safe_filename)
            shutil.copy2(file_path, target_path)

            # 3. å¤„ç†PCAPæ–‡ä»¶
            df = self.processor.read_pcap_advanced(target_path)

            if len(df) == 0:
                return {
                    "success": False,
                    "error": "PCAPæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç½‘ç»œæµ",
                    "code": "NO_FLOWS",
                }

            # 4. ç”ŸæˆCSVè¾“å‡º
            csv_filename = f"{Path(safe_filename).stem}_processed.csv"
            csv_path = os.path.join(self.output_dir, csv_filename)
            df.to_csv(csv_path, index=False)

            # 5. ç”Ÿæˆå¤„ç†æ‘˜è¦
            summary = self._generate_processing_summary(df, safe_filename)

            # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            # os.remove(target_path)

            logger.info(f"PCAPå¤„ç†å®Œæˆ: {len(df)} ä¸ªç½‘ç»œæµ")

            return {
                "success": True,
                "processed_flows": len(df),
                "csv_file": csv_path,
                "csv_filename": csv_filename,
                "summary": summary,
                "original_filename": filename,
            }

        except Exception as e:
            logger.error(f"å¤„ç†PCAPæ–‡ä»¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "code": "PCAP_PROCESSING_ERROR"}
            summary = self._generate_processing_summary(df, safe_filename)

            # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            # os.remove(target_path)

            logger.info(f"å¤„ç†å®Œæˆ: {len(df)} ä¸ªç½‘ç»œæµ")

            return {
                "success": True,
                "processed_flows": len(df),
                "csv_file": csv_path,
                "csv_filename": csv_filename,
                "summary": summary,
                "original_filename": filename,
            }

        except Exception as e:
            logger.error(f"å¤„ç†PCAPæ–‡ä»¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "code": "PROCESSING_ERROR"}

    def _validate_pcap_file(self, file_path: str) -> bool:
        """éªŒè¯PCAPæ–‡ä»¶æ ¼å¼"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            ext = Path(file_path).suffix.lower()
            if ext not in [".pcap", ".pcapng"]:
                return False

            # æ£€æŸ¥æ–‡ä»¶é­”æ•°
            with open(file_path, "rb") as f:
                magic = f.read(4)
                # PCAPé­”æ•°
                pcap_magic = [b"\xd4\xc3\xb2\xa1", b"\xa1\xb2\xc3\xd4"]
                pcapng_magic = b"\x0a\x0d\x0d\x0a"

                return magic in pcap_magic or magic == pcapng_magic

        except Exception:
            return False

    def _generate_safe_filename(self, filename: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # ç§»é™¤å±é™©å­—ç¬¦
        safe_chars = (
            "-_.() abcdefghijklmnopqrstuvwxyz" + "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
        )
        safe_filename = "".join(c for c in filename if c in safe_chars)

        # æ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
        timestamp = int(time.time())
        name, ext = os.path.splitext(safe_filename)
        return f"{name}_{timestamp}{ext}"

    def _generate_processing_summary(
        self, df: pd.DataFrame, filename: str
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        return {
            "filename": filename,
            "total_flows": len(df),
            "protocol_distribution": df["proto"].value_counts().head(10).to_dict(),
            "service_distribution": df["service"].value_counts().head(10).to_dict(),
            "duration_stats": {
                "mean": float(df["dur"].mean()),
                "median": float(df["dur"].median()),
                "max": float(df["dur"].max()),
                "min": float(df["dur"].min()),
            },
            "packet_stats": {
                "total_packets": int((df["spkts"] + df["dpkts"]).sum()),
                "avg_packets_per_flow": float((df["spkts"] + df["dpkts"]).mean()),
            },
            "byte_stats": {
                "total_bytes": int((df["sbytes"] + df["dbytes"]).sum()),
                "avg_bytes_per_flow": float((df["sbytes"] + df["dbytes"]).mean()),
            },
        }


def create_web_api():
    """åˆ›å»ºWeb APIæ¥å£ç¤ºä¾‹"""
    try:
        from flask import Flask, request, jsonify, send_file
        from werkzeug.utils import secure_filename
    except ImportError:
        logger.warning("Flaskæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºWeb API")
        return None

    app = Flask(__name__)
    app.config["MAX_CONTENT_LENGTH"] = None  # æ— é™åˆ¶ï¼Œå…è®¸å¤„ç†å¤§æ–‡ä»¶

    handler = FrontendPcapHandler()

    @app.route("/upload_pcap", methods=["POST"])
    def upload_pcap():
        """å•æ–‡ä»¶ä¸Šä¼ æ¥å£"""
        try:
            if "file" not in request.files:
                return jsonify({"success": False, "error": "æ²¡æœ‰æ–‡ä»¶"}), 400

            file = request.files["file"]
            if file.filename == "":
                return jsonify({"success": False, "error": "æ–‡ä»¶åä¸ºç©º"}), 400

            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = os.path.join(
                tempfile.gettempdir(), secure_filename(file.filename)
            )
            file.save(temp_path)

            # å¤„ç†æ–‡ä»¶
            result = handler.handle_uploaded_pcap(temp_path, file.filename)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)

            return jsonify(result)

        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

    @app.route("/download_csv/<filename>")
    def download_csv(filename):
        """ä¸‹è½½å¤„ç†ç»“æœ"""
        try:
            file_path = os.path.join(handler.output_dir, filename)
            if os.path.exists(file_path):
                return send_file(file_path, as_attachment=True)
            else:
                return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    return app


def command_line_interface():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="PCAPæ–‡ä»¶å¤„ç†å·¥å…·")
    parser.add_argument("input_file", help="è¾“å…¥PCAPæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-v", "--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        handler = FrontendPcapHandler()
        result = handler.handle_uploaded_pcap(
            args.input_file, os.path.basename(args.input_file)
        )

        if result["success"]:
            print(f"âœ… å¤„ç†æˆåŠŸ!")
            print(f"å¤„ç†äº† {result['processed_flows']} ä¸ªç½‘ç»œæµ")
            print(f"è¾“å‡ºæ–‡ä»¶: {result['csv_file']}")

            if args.output:
                shutil.copy2(result["csv_file"], args.output)
                print(f"ç»“æœå·²å¤åˆ¶åˆ°: {args.output}")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        command_line_interface()
    else:
        # æä¾›ä½¿ç”¨è¯´æ˜ï¼ˆå»¶è¿Ÿåˆ°å‡½æ•°å®šä¹‰ä¹‹åï¼‰
        pass


class PcapToCSVConverter:
    """PCAPåˆ°CSVè½¬æ¢å™¨ - ä¼ä¸šçº§æ•°æ®å¤„ç†ç®¡é“"""

    def __init__(self, output_dir: str = "processed_data"):
        self.output_dir = output_dir
        self.processor = AdvancedPcapProcessor()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

    def convert_batch(self, pcap_files: List[str], output_filename: str = None) -> str:
        """
        æ‰¹é‡è½¬æ¢PCAPæ–‡ä»¶åˆ°CSV

        Args:
            pcap_files: PCAPæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_filename: è¾“å‡ºCSVæ–‡ä»¶å

        Returns:
            è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        """
        all_data = []

        for pcap_file in pcap_files:
            logger.info(f"å¤„ç†æ–‡ä»¶: {pcap_file}")
            try:
                df = self.processor.read_pcap_advanced(pcap_file)
                df["source_file"] = os.path.basename(pcap_file)
                all_data.append(df)
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {pcap_file}: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•PCAPæ–‡ä»¶")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)

        # é‡æ–°åˆ†é…ID
        combined_df["id"] = range(1, len(combined_df) + 1)

        # ä¿å­˜åˆ°CSV
        if output_filename is None:
            output_filename = f"converted_traffic_{int(time.time())}.csv"

        output_path = os.path.join(self.output_dir, output_filename)
        combined_df.to_csv(output_path, index=False)

        logger.info(f"è½¬æ¢å®Œæˆ: {len(combined_df)} æ¡è®°å½•ä¿å­˜åˆ° {output_path}")
        return output_path

    def convert_single(self, pcap_file: str, output_filename: str = None) -> str:
        """
        è½¬æ¢å•ä¸ªPCAPæ–‡ä»¶

        Args:
            pcap_file: PCAPæ–‡ä»¶è·¯å¾„
            output_filename: è¾“å‡ºCSVæ–‡ä»¶å

        Returns:
            è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        """
        return self.convert_batch([pcap_file], output_filename)

    def get_conversion_summary(self, csv_file: str) -> Dict[str, Any]:
        """è·å–è½¬æ¢ç»“æœæ‘˜è¦"""
        df = pd.read_csv(csv_file)

        summary = {
            "total_flows": len(df),
            "protocol_distribution": df["proto"].value_counts().to_dict(),
            "service_distribution": df["service"].value_counts().to_dict(),
            "duration_stats": {
                "mean": df["dur"].mean(),
                "median": df["dur"].median(),
                "max": df["dur"].max(),
                "min": df["dur"].min(),
            },
            "packet_stats": {
                "mean_packets_per_flow": (df["spkts"] + df["dpkts"]).mean(),
                "total_packets": (df["spkts"] + df["dpkts"]).sum(),
            },
            "byte_stats": {
                "mean_bytes_per_flow": (df["sbytes"] + df["dbytes"]).mean(),
                "total_bytes": (df["sbytes"] + df["dbytes"]).sum(),
            },
        }

        return summary


def demonstrate_pcap_processing():
    """æ¼”ç¤ºPCAPå¤„ç†åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ”§ AIç™½åå•æµé‡è¿‡æ»¤ç³»ç»Ÿ - æ•°æ®é¢„å¤„ç†æ¨¡å—æ¼”ç¤º")
    print("=" * 60)

    print("\nğŸ“‹ æ”¯æŒçš„åŠŸèƒ½:")
    print("âœ… 1. PCAPæ–‡ä»¶è¯»å–å’Œè§£æ")
    print("âœ… 2. ç½‘ç»œæµé‡è‡ªåŠ¨åˆ†å‰²")
    print("âœ… 3. ä¸°å¯Œç‰¹å¾æå–å·¥ç¨‹")
    print("âœ… 4. æ•°æ®æ¸…æ´—å’Œå»å™ª")
    print("âœ… 5. æ ¼å¼è½¬æ¢ (PCAP â†’ CSV)")
    print("âœ… 6. UNSW-NB15æ ¼å¼å…¼å®¹")

    print("\nğŸ—ï¸ å¤„ç†æµç¨‹:")
    print("1. ğŸ“Š æµé‡åˆ†å‰²: å°†æ•°æ®åŒ…æŒ‰è¿æ¥åˆ†ç»„")
    print("2. ğŸ” ç‰¹å¾æå–: æå–47ç»´ç½‘ç»œç‰¹å¾")
    print("3. ğŸ§¹ æ•°æ®æ¸…æ´—: ç§»é™¤å™ªå£°å’Œå¼‚å¸¸å€¼")
    print("4. ğŸ“ æ ¼å¼è½¬æ¢: ç”Ÿæˆæ ‡å‡†CSVæ–‡ä»¶")

    print("\nâš™ï¸ æ ¸å¿ƒæŠ€æœ¯:")
    print("â€¢ Scapy: æ·±åº¦åŒ…è§£æå’Œåè®®è¯†åˆ«")
    print("â€¢ Pandas: é«˜æ•ˆæ•°æ®å¤„ç†å’Œæ¸…æ´—")
    print("â€¢ NumPy: æ•°å€¼è®¡ç®—å’Œç»Ÿè®¡åˆ†æ")
    print("â€¢ æµé‡åˆ†å‰²ç®—æ³•: æ™ºèƒ½è¿æ¥è¯†åˆ«")

    print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("```python")
    print("# 1. å¤„ç†å•ä¸ªPCAPæ–‡ä»¶")
    print("processor = AdvancedPcapProcessor()")
    print("df = processor.read_pcap_advanced('traffic.pcap')")
    print("")
    print("# 2. æ‰¹é‡è½¬æ¢PCAPåˆ°CSV")
    print("converter = PcapToCSVConverter()")
    print("csv_file = converter.convert_batch(['file1.pcap', 'file2.pcap'])")
    print("")
    print("# 3. åŠ è½½åˆ°AIæ¨¡å‹")
    print("train_loader, test_loader, input_dim, scaler = load_train_test(")
    print("    'traffic.csv', 'test.csv')")
    print("```")

    print("\nğŸ“Š è¾“å‡ºç‰¹å¾ (å…¼å®¹UNSW-NB15):")
    features = [
        "dur, proto, service, state",
        "spkts, dpkts, sbytes, dbytes",
        "rate, sload, dload",
        "smean, dmean, sinpkt, dinpkt",
        "sjit, djit, sttl, dttl",
        "ä»¥åŠæ›´å¤šç½‘ç»œç»Ÿè®¡ç‰¹å¾...",
    ]
    for feature in features:
        print(f"   â€¢ {feature}")

    print("\n" + "=" * 60)
    print("ğŸ¯ è¯¥æ¨¡å—ç°å·²æ”¯æŒå®Œæ•´çš„PCAPå¤„ç†æµç¨‹!")
    print("å¯ç›´æ¥ç”¨äºAIæ¨¡å‹è®­ç»ƒå’Œå®æ—¶æµé‡æ£€æµ‹ã€‚")
    print("=" * 60)


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demonstrate_pcap_processing()

    # å¦‚æœæœ‰PCAPæ–‡ä»¶ï¼Œå¯ä»¥è¿›è¡Œå®é™…æµ‹è¯•
    # è¿™é‡Œæ·»åŠ æµ‹è¯•ä»£ç ç¤ºä¾‹
    print("\nğŸ§ª æµ‹è¯•ç¤ºä¾‹:")
    print("å¦‚éœ€æµ‹è¯•å®é™…PCAPæ–‡ä»¶å¤„ç†ï¼Œè¯·å°†PCAPæ–‡ä»¶è·¯å¾„ä¼ å…¥ä»¥ä¸‹ä»£ç :")
    print("```python")
    print("processor = AdvancedPcapProcessor()")
    print("df = processor.read_pcap_advanced('your_file.pcap')")
    print("print(f'å¤„ç†ç»“æœ: {len(df)} ä¸ªç½‘ç»œæµ')")
    print("print(df.head())")
    print("```")


def load_data_for_inference(
    csv_path: str, max_samples: int = None
) -> Tuple[torch.Tensor, pd.DataFrame]:
    """
    ä¸ºæ¨ç†åŠ è½½æ•°æ®

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        max_samples: æœ€å¤§æ ·æœ¬æ•°é‡

    Returns:
        (ç‰¹å¾å¼ é‡, åŸå§‹DataFrame)
    """
    try:
        # åŠ è½½CSVæ•°æ®
        df = pd.read_csv(csv_path)

        if max_samples and len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42)

        # é¢„å¤„ç†æ•°æ®
        X, y = preprocess_df(df, drop_service=True)

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # è½¬æ¢ä¸ºå¼ é‡
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

        return X_tensor, df

    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise


def prepare_inference_data(data_path: str) -> Dict[str, Any]:
    """
    å‡†å¤‡æ¨ç†æ•°æ®çš„ç»Ÿä¸€æ¥å£

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒCSVã€PCAPï¼‰

    Returns:
        åŒ…å«å¼ é‡å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    ext = Path(data_path).suffix.lower()

    if ext == ".csv":
        X_tensor, df = load_data_for_inference(data_path)
    elif ext in [".pcap", ".pcapng"]:
        # å…ˆè½¬æ¢PCAPä¸ºDataFrame
        processor = AdvancedPcapProcessor()
        df = processor.read_pcap_advanced(data_path)

        # ç„¶åå‡†å¤‡æ¨ç†æ•°æ®
        X, y = preprocess_df(df, drop_service=True)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    return {
        "features": X_tensor,
        "dataframe": df,
        "total_samples": len(df),
        "feature_dim": X_tensor.shape[1] if len(X_tensor.shape) > 1 else 1,
        "file_path": data_path,
    }


# ç°åœ¨åœ¨è¿™é‡Œè°ƒç”¨æ¼”ç¤ºå‡½æ•°
if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        command_line_interface()
    else:
        # è¿è¡Œæ¼”ç¤º
        demonstrate_pcap_processing()

        # æä¾›ä½¿ç”¨è¯´æ˜
        print("\nğŸŒ å‰ç«¯æ¥å£æ”¯æŒ:")
        print("1. ğŸ“¤ å•æ–‡ä»¶ä¸Šä¼ : FrontendPcapHandler.handle_uploaded_pcap()")
        print("2. ğŸŒ Web API: create_web_api() åˆ›å»ºFlaskæ¥å£")
        print("3. ğŸ’» å‘½ä»¤è¡Œ: python unsw_nb15_preprocess.py <pcap_file>")

        print("\nğŸ“ å‰ç«¯é›†æˆç¤ºä¾‹:")
        print("```python")
        print("handler = FrontendPcapHandler()")
        print(
            "result = handler.handle_uploaded_pcap('/tmp/upload.pcap', 'traffic.pcap')"
        )
        print("if result['success']:")
        print("    print(f'å¤„ç†äº† {result[\"processed_flows\"]} ä¸ªæµ')")
        print("    csv_file = result['csv_file']")
        print("```")
