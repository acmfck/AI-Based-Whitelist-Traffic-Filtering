#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„æœåŠ¡å™¨å¯åŠ¨è„šæœ¬ï¼Œç”¨äºæµ‹è¯•æŒ‰é’®åŠŸèƒ½
"""

import os
import sys
import gc
import tempfile
import json
import sqlite3
from datetime import datetime
from flask import (
    Flask,
    render_template,
    request,
    jsonify,
    send_file,
    flash,
    redirect,
    url_for,
)
from werkzeug.utils import secure_filename

# å¯¼å…¥ç³»ç»Ÿç›‘æ§åº“
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("âš ï¸ psutilæœªå®‰è£…ï¼Œå†…å­˜ç›‘æ§åŠŸèƒ½å°†ä¸å¯ç”¨")

# æ·»åŠ æ•°æ®å¤„ç†æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), "data"))

# å°è¯•å¯¼å…¥å¤„ç†æ¨¡å—
try:
    from data.unsw_nb15_preprocess import FrontendPcapHandler

    print("âœ… æˆåŠŸå¯¼å…¥ FrontendPcapHandler")
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥ FrontendPcapHandler å¤±è´¥: {e}")

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£ç±»
    class FrontendPcapHandler:
        def __init__(self, upload_dir="uploads", output_dir="processed"):
            self.upload_dir = upload_dir
            self.output_dir = output_dir
            os.makedirs(upload_dir, exist_ok=True)
            os.makedirs(output_dir, exist_ok=True)

        def handle_uploaded_pcap(self, file_path, filename):
            return {
                "success": True,
                "processed_flows": 100,
                "csv_file": f"{self.output_dir}/test_output.csv",
                "processing_time": 1.5,
                "message": "æµ‹è¯•å¤„ç†å®Œæˆ",
            }


app = Flask(__name__)
app.secret_key = "ai_traffic_filter_secret_key_2024"
# ç§»é™¤æ–‡ä»¶å¤§å°é™åˆ¶ï¼Œå…è®¸å¤„ç†å¤§æ–‡ä»¶
app.config["MAX_CONTENT_LENGTH"] = None  # æ— é™åˆ¶

# å…è®¸çš„æ–‡ä»¶æ‰©å±•å
ALLOWED_EXTENSIONS = {"pcap", "pcapng", "csv"}

# åˆ›å»ºå¤„ç†å™¨å®ä¾‹
handler = FrontendPcapHandler(upload_dir="uploads", output_dir="processed")

# æ•°æ®åº“é…ç½®
DATABASE_PATH = "analysis_history.db"


def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # åˆ›å»ºåˆ†æå†å²è®°å½•è¡¨
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS analysis_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_id TEXT UNIQUE NOT NULL,
            filename TEXT NOT NULL,
            file_size INTEGER,
            upload_time TIMESTAMP,
            analysis_time TIMESTAMP,
            processing_time REAL,
            total_flows INTEGER,
            anomaly_count INTEGER,
            security_level TEXT,
            cpu_usage REAL,
            memory_usage REAL,
            analysis_results TEXT,  -- JSONæ ¼å¼å­˜å‚¨å®Œæ•´åˆ†æç»“æœ
            status TEXT DEFAULT 'pending'  -- pending, completed, failed
        )
    """
    )

    # åˆ›å»ºå¿«é€ŸæŸ¥è¯¢ç´¢å¼•
    cursor.execute(
        "CREATE INDEX IF NOT EXISTS idx_upload_time ON analysis_history(upload_time)"
    )
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_status ON analysis_history(status)")
    cursor.execute(
        "CREATE INDEX IF NOT EXISTS idx_filename ON analysis_history(filename)"
    )

    # ä¿®å¤ç°æœ‰è®°å½•çš„æ—¶åŒºé—®é¢˜ï¼Œå¹¶ç»Ÿä¸€æ—¶é—´æ ¼å¼ä¸º YYYY/M/D H:M:S
    try:
        # å…ˆå°†æ‰€æœ‰æ—¶é—´è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œç„¶åç»Ÿä¸€ä¸ºæœ¬åœ°æ—¶é—´æ˜¾ç¤ºæ ¼å¼
        cursor.execute(
            """
            UPDATE analysis_history 
            SET upload_time = strftime('%Y/%m/%d %H:%M:%S', 
                datetime(upload_time, 'localtime'))
            WHERE upload_time IS NOT NULL
        """
        )
        cursor.execute(
            """
            UPDATE analysis_history 
            SET analysis_time = strftime('%Y/%m/%d %H:%M:%S', 
                datetime(analysis_time, 'localtime'))
            WHERE analysis_time IS NOT NULL
        """
        )
        print("âœ… æ•°æ®åº“æ—¶åŒºå’Œæ ¼å¼ä¿®å¤å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ æ—¶åŒºä¿®å¤è­¦å‘Š: {e}")

    conn.commit()
    conn.close()


def save_analysis_record(
    task_id, filename, file_size, analysis_results=None, status="pending"
):
    """ä¿å­˜åˆ†æè®°å½•åˆ°æ•°æ®åº“"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        # è·å–æœ¬åœ°æ—¶é—´ - ä½¿ç”¨ä¸å‰ç«¯ä¸€è‡´çš„æ ¼å¼
        current_time = datetime.now().strftime("%Y/%m/%d %H:%M:%S")

        if status == "pending":
            # æ’å…¥åˆå§‹è®°å½•ï¼Œä½¿ç”¨æœ¬åœ°æ—¶é—´
            cursor.execute(
                """
                INSERT OR REPLACE INTO analysis_history 
                (task_id, filename, file_size, upload_time, status)
                VALUES (?, ?, ?, ?, ?)
            """,
                (task_id, filename, file_size, current_time, status),
            )
        else:
            # æ›´æ–°å®Œæˆçš„åˆ†æç»“æœ
            total_flows = (
                analysis_results.get("basic_info", {}).get("total_flows", 0)
                if analysis_results
                else 0
            )
            # è·å–å¤„ç†æ—¶é—´ - å…¼å®¹å¤šç§å­—æ®µå
            processing_time = 0
            if analysis_results:
                basic_info = analysis_results.get("basic_info", {})
                processing_time = basic_info.get(
                    "total_processing_time", 0
                ) or basic_info.get("processing_time", 0)

            anomaly_count = (
                analysis_results.get("detection_results", {})
                .get("anomaly_detection", {})
                .get("anomalies_detected", 0)
                if analysis_results
                else 0
            )

            # è®¡ç®—å®‰å…¨ç­‰çº§
            security_level = "safe"
            if analysis_results and analysis_results.get("attack_analysis", {}).get(
                "label_distribution"
            ):
                attack_percentage = analysis_results["attack_analysis"][
                    "label_distribution"
                ].get("Attack_Percentage", 0)
                if attack_percentage > 30:
                    security_level = "high_risk"
                elif attack_percentage > 10:
                    security_level = "medium_risk"

            cursor.execute(
                """
                UPDATE analysis_history 
                SET analysis_time = ?,
                    processing_time = ?,
                    total_flows = ?,
                    anomaly_count = ?,
                    security_level = ?,
                    analysis_results = ?,
                    status = ?
                WHERE task_id = ?
            """,
                (
                    current_time,  # ä½¿ç”¨æœ¬åœ°æ—¶é—´
                    processing_time,
                    total_flows,
                    anomaly_count,
                    security_level,
                    json.dumps(analysis_results) if analysis_results else None,
                    status,
                    task_id,
                ),
            )

        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ†æè®°å½•å¤±è´¥: {e}")
        return False


def get_analysis_history(limit=50, offset=0, status_filter=None):
    """è·å–åˆ†æå†å²è®°å½•"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        query = """
            SELECT id, task_id, filename, file_size, upload_time, analysis_time,
                   processing_time, total_flows, anomaly_count, security_level,
                   status
            FROM analysis_history 
        """
        params = []

        if status_filter:
            query += " WHERE status = ?"
            params.append(status_filter)

        query += " ORDER BY upload_time DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        cursor.execute(query, params)
        records = cursor.fetchall()

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        history = []
        for record in records:
            history.append(
                {
                    "id": record[0],
                    "task_id": record[1],
                    "filename": record[2],
                    "file_size": record[3],
                    "upload_time": record[4],
                    "analysis_time": record[5],
                    "processing_time": record[6],
                    "total_flows": record[7],
                    "anomaly_count": record[8],
                    "security_level": record[9],
                    "status": record[10],
                }
            )

        conn.close()
        return history
    except Exception as e:
        print(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return []


def get_analysis_detail(task_id):
    """è·å–ç‰¹å®šåˆ†æçš„è¯¦ç»†ç»“æœ"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT analysis_results, filename, upload_time, processing_time
            FROM analysis_history 
            WHERE task_id = ?
        """,
            (task_id,),
        )

        record = cursor.fetchone()
        conn.close()

        if record and record[0]:
            return {
                "analysis_results": json.loads(record[0]),
                "filename": record[1],
                "upload_time": record[2],
                "processing_time": record[3],
            }
        return None
    except Exception as e:
        print(f"âŒ è·å–åˆ†æè¯¦æƒ…å¤±è´¥: {e}")
        return None


def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦è¢«å…è®¸"""
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS


@app.route("/")
def index():
    """ä¸»é¡µé¢ - æ–°çš„AIæ£€æµ‹ä»ªè¡¨æ¿"""
    return render_template("ai_detection_dashboard.html")


@app.route("/legacy")
def legacy():
    """ä¼ ç»Ÿæµ‹è¯•é¡µé¢"""
    return render_template("inline_test.html")


@app.route("/test")
def button_test():
    """æŒ‰é’®æµ‹è¯•é¡µé¢"""
    return render_template("button_test.html")


@app.route("/quick")
def quick_test():
    """å¿«é€Ÿæµ‹è¯•é¡µé¢"""
    return render_template("quick_test.html")


@app.route("/api/history")
def api_history():
    """è·å–åˆ†æå†å²è®°å½•API"""
    try:
        page = int(request.args.get("page", 1))
        limit = int(request.args.get("limit", 20))
        status_filter = request.args.get("status")

        offset = (page - 1) * limit
        history = get_analysis_history(
            limit=limit, offset=offset, status_filter=status_filter
        )

        return jsonify({"success": True, "data": history, "page": page, "limit": limit})
    except Exception as e:
        print(f"âŒ è·å–å†å²è®°å½•APIå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/history/<task_id>")
def api_history_detail(task_id):
    """è·å–ç‰¹å®šåˆ†æçš„è¯¦ç»†ç»“æœAPI"""
    try:
        detail = get_analysis_detail(task_id)
        if detail:
            return jsonify({"success": True, "data": detail})
        else:
            return jsonify({"success": False, "error": "è®°å½•ä¸å­˜åœ¨"}), 404
    except Exception as e:
        print(f"âŒ è·å–å†å²è¯¦æƒ…APIå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/history/<task_id>", methods=["DELETE"])
def api_delete_history(task_id):
    """åˆ é™¤å†å²è®°å½•API"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM analysis_history WHERE task_id = ?", (task_id,))
        conn.commit()
        conn.close()

        return jsonify({"success": True, "message": "è®°å½•å·²åˆ é™¤"})
    except Exception as e:
        print(f"âŒ åˆ é™¤å†å²è®°å½•å¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/batch_clear", methods=["POST"])
def api_batch_clear():
    """æ‰¹é‡æ¸…ç©ºåŠŸèƒ½API"""
    print(f"ğŸ§¹ æ‰¹é‡æ¸…ç©ºè¯·æ±‚ - æ—¶é—´: {datetime.now()}")

    try:
        data = request.get_json() or {}
        clear_types = data.get("types", [])

        results = {"success": True, "cleared": {}, "errors": []}

        # æ¸…ç©ºå†å²è®°å½•
        if "history" in clear_types:
            try:
                conn = sqlite3.connect(DATABASE_PATH)
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM analysis_history")
                count = cursor.fetchone()[0]
                cursor.execute("DELETE FROM analysis_history")
                conn.commit()
                conn.close()
                results["cleared"]["history"] = f"å·²æ¸…ç©º {count} æ¡å†å²è®°å½•"
                print(f"âœ… æ¸…ç©ºå†å²è®°å½•: {count} æ¡")
            except Exception as e:
                error_msg = f"æ¸…ç©ºå†å²è®°å½•å¤±è´¥: {str(e)}"
                results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        # æ¸…ç©ºä¸Šä¼ æ–‡ä»¶
        if "uploads" in clear_types:
            try:
                upload_dir = handler.upload_dir
                if os.path.exists(upload_dir):
                    file_count = 0
                    for filename in os.listdir(upload_dir):
                        file_path = os.path.join(upload_dir, filename)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                            file_count += 1
                    results["cleared"]["uploads"] = f"å·²æ¸…ç©º {file_count} ä¸ªä¸Šä¼ æ–‡ä»¶"
                    print(f"âœ… æ¸…ç©ºä¸Šä¼ æ–‡ä»¶: {file_count} ä¸ª")
                else:
                    results["cleared"]["uploads"] = "ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º"
            except Exception as e:
                error_msg = f"æ¸…ç©ºä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}"
                results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        # æ¸…ç©ºå¤„ç†ç»“æœ
        if "processed" in clear_types:
            try:
                processed_dir = handler.output_dir
                if os.path.exists(processed_dir):
                    file_count = 0
                    for filename in os.listdir(processed_dir):
                        file_path = os.path.join(processed_dir, filename)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                            file_count += 1
                    results["cleared"][
                        "processed"
                    ] = f"å·²æ¸…ç©º {file_count} ä¸ªå¤„ç†ç»“æœæ–‡ä»¶"
                    print(f"âœ… æ¸…ç©ºå¤„ç†ç»“æœ: {file_count} ä¸ª")
                else:
                    results["cleared"]["processed"] = "å¤„ç†ç»“æœç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º"
            except Exception as e:
                error_msg = f"æ¸…ç©ºå¤„ç†ç»“æœå¤±è´¥: {str(e)}"
                results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        # æ¸…ç©ºä¸´æ—¶æ–‡ä»¶
        if "temp" in clear_types:
            try:
                temp_count = 0
                # æ¸…ç©ºPythonä¸´æ—¶ç›®å½•ä¸­çš„ç›¸å…³æ–‡ä»¶
                temp_dir = tempfile.gettempdir()
                for filename in os.listdir(temp_dir):
                    if filename.startswith("ai_traffic_") or filename.startswith(
                        "pcap_"
                    ):
                        file_path = os.path.join(temp_dir, filename)
                        try:
                            if os.path.isfile(file_path):
                                os.remove(file_path)
                                temp_count += 1
                        except:
                            continue  # å¿½ç•¥æ— æ³•åˆ é™¤çš„æ–‡ä»¶

                # æ¸…ç©ºå½“å‰ç›®å½•ä¸‹çš„ä¸´æ—¶æ–‡ä»¶
                current_dir = os.path.dirname(__file__)
                for filename in os.listdir(current_dir):
                    if filename.endswith(".tmp") or filename.startswith("temp_"):
                        file_path = os.path.join(current_dir, filename)
                        try:
                            if os.path.isfile(file_path):
                                os.remove(file_path)
                                temp_count += 1
                        except:
                            continue

                results["cleared"]["temp"] = f"å·²æ¸…ç©º {temp_count} ä¸ªä¸´æ—¶æ–‡ä»¶"
                print(f"âœ… æ¸…ç©ºä¸´æ—¶æ–‡ä»¶: {temp_count} ä¸ª")
            except Exception as e:
                error_msg = f"æ¸…ç©ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}"
                results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        if "memory" in clear_types:
            try:
                collected = gc.collect()
                results["cleared"]["memory"] = f"å·²é‡Šæ”¾ {collected} ä¸ªå¯¹è±¡çš„å†…å­˜"
                print(f"âœ… åƒåœ¾å›æ”¶: {collected} ä¸ªå¯¹è±¡")
            except Exception as e:
                error_msg = f"å†…å­˜æ¸…ç†å¤±è´¥: {str(e)}"
                results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if results["errors"]:
            results["success"] = False

        # è®°å½•æ¸…ç©ºæ“ä½œ
        print(f"ğŸ§¹ æ‰¹é‡æ¸…ç©ºå®Œæˆ - æ¸…ç©ºç±»å‹: {clear_types}")

        return jsonify(results)

    except Exception as e:
        print(f"âŒ æ‰¹é‡æ¸…ç©ºæ“ä½œå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": f"æ‰¹é‡æ¸…ç©ºæ“ä½œå¤±è´¥: {str(e)}"}), 500


@app.route("/api/system_info", methods=["GET"])
def api_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯æ¥å£"""
    try:
        print("ğŸ“Š è·å–ç³»ç»Ÿä¿¡æ¯è¯·æ±‚")

        # è·å–å†å²è®°å½•æ•°é‡
        history_count = 0
        try:
            conn = sqlite3.connect(DATABASE_PATH)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM analysis_history")
            history_count = cursor.fetchone()[0]
            conn.close()
        except Exception as e:
            print(f"è·å–å†å²è®°å½•æ•°é‡å¤±è´¥: {e}")

        # è·å–ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯
        uploads_count = 0
        uploads_size = 0
        upload_dir = "uploads"
        if os.path.exists(upload_dir):
            try:
                for filename in os.listdir(upload_dir):
                    file_path = os.path.join(upload_dir, filename)
                    if os.path.isfile(file_path):
                        uploads_count += 1
                        uploads_size += os.path.getsize(file_path)
            except Exception as e:
                print(f"è·å–ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")

        # è·å–å¤„ç†ç»“æœæ–‡ä»¶ä¿¡æ¯
        processed_count = 0
        processed_size = 0
        processed_dir = "processed"
        if os.path.exists(processed_dir):
            try:
                for filename in os.listdir(processed_dir):
                    file_path = os.path.join(processed_dir, filename)
                    if os.path.isfile(file_path):
                        processed_count += 1
                        processed_size += os.path.getsize(file_path)
            except Exception as e:
                print(f"è·å–å¤„ç†ç»“æœæ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")

        # è·å–ä¸´æ—¶æ–‡ä»¶ä¿¡æ¯
        temp_count = 0
        temp_size = 0
        try:
            current_dir = os.path.dirname(__file__)
            for filename in os.listdir(current_dir):
                if filename.endswith(".tmp") or filename.startswith("temp_"):
                    file_path = os.path.join(current_dir, filename)
                    if os.path.isfile(file_path):
                        temp_count += 1
                        temp_size += os.path.getsize(file_path)
        except Exception as e:
            print(f"è·å–ä¸´æ—¶æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")

        # è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯
        memory_info = {}
        if PSUTIL_AVAILABLE:
            try:
                # ç³»ç»Ÿå†…å­˜ä¿¡æ¯
                system_memory = psutil.virtual_memory()
                memory_info = {
                    "total": system_memory.total,
                    "available": system_memory.available,
                    "used": system_memory.used,
                    "percent": system_memory.percent,
                    "free": system_memory.free,
                }

                # å½“å‰è¿›ç¨‹å†…å­˜ä¿¡æ¯
                current_process = psutil.Process()
                process_memory = current_process.memory_info()
                memory_info["process"] = {
                    "rss": process_memory.rss,  # å®é™…ç‰©ç†å†…å­˜
                    "vms": process_memory.vms,  # è™šæ‹Ÿå†…å­˜
                    "percent": current_process.memory_percent(),
                }
            except Exception as e:
                print(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
                memory_info = {"error": "æ— æ³•è·å–å†…å­˜ä¿¡æ¯"}

        system_info = {
            "history_count": history_count,
            "uploads_count": uploads_count,
            "uploads_size": uploads_size,
            "processed_count": processed_count,
            "processed_size": processed_size,
            "temp_count": temp_count,
            "temp_size": temp_size,
            "memory": memory_info,
        }

        print(f"âœ… ç³»ç»Ÿä¿¡æ¯: {system_info}")

        return jsonify({"success": True, "info": system_info})

    except Exception as e:
        print(f"âŒ è·å–ç³»ç»Ÿä¿¡æ¯å¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"}), 500


@app.route("/api/status")
def api_status():
    """APIçŠ¶æ€æ£€æŸ¥æ¥å£"""
    print("ğŸ“Š APIçŠ¶æ€æ£€æŸ¥è¯·æ±‚")
    return jsonify(
        {
            "status": "ok",
            "message": "æœåŠ¡å™¨è¿è¡Œæ­£å¸¸",
            "timestamp": datetime.now().isoformat(),
            "upload_enabled": True,
        }
    )


@app.route("/api/upload", methods=["POST"])
def api_upload():
    """APIæ¥å£ - ä¸Šä¼ æ–‡ä»¶å¹¶è¿›è¡Œåˆ†æ"""
    print(f"ğŸ”„ APIä¸Šä¼ è¯·æ±‚å¼€å§‹ - æ—¶é—´: {datetime.now()}")
    print(f"ğŸ“„ è¯·æ±‚æ–¹æ³•: {request.method}")
    print(f"ğŸ“¦ è¯·æ±‚æ–‡ä»¶: {list(request.files.keys())}")

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if "file" not in request.files:
            print("âŒ é”™è¯¯: è¯·æ±‚ä¸­æ²¡æœ‰æ‰¾åˆ° 'file' å­—æ®µ")
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400

        file = request.files["file"]
        print(f"ğŸ“ ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯: æ–‡ä»¶å={file.filename}")

        if file.filename == "":
            print("âŒ é”™è¯¯: æ–‡ä»¶åä¸ºç©º")
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400

        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_filename = f"{timestamp}_{filename}"
            upload_path = os.path.join(handler.upload_dir, unique_filename)

            print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶è·¯å¾„: {upload_path}")

            # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
            os.makedirs(handler.upload_dir, exist_ok=True)

            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file.save(upload_path)
            print(f"âœ… æ–‡ä»¶å·²ä¿å­˜: {upload_path}")

            # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
            task_id = f"task_{timestamp}"
            print(f"ğŸ†” ä»»åŠ¡ID: {task_id}")

            # ç®€åŒ–çš„å¤„ç†ç»“æœ
            result = handler.handle_uploaded_pcap(upload_path, unique_filename)

            if result["success"]:
                print("âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ")

                # æ„å»ºè¿”å›ç»“æœ
                analysis_summary = {
                    "task_id": task_id,
                    "filename": unique_filename,
                    "file_id": unique_filename.replace(".", "_"),
                    "processed_flows": result["processed_flows"],
                    "processing_time": result.get("processing_time", 0),
                    "ai_analysis": True,
                    "ai_summary": {
                        "total_flows": result["processed_flows"],
                        "features": 42,
                    },
                    "anomaly_detection": {
                        "anomalies_detected": 0,
                        "anomaly_percentage": 0,
                        "status": "completed",
                    },
                    "generated_files": [
                        {
                            "name": "test_chart.png",
                            "type": "png",
                            "url": "/api/download/test_chart.png",
                        }
                    ],
                }

                return jsonify(
                    {
                        "success": True,
                        "message": "æ–‡ä»¶å¤„ç†å®Œæˆ",
                        "result": analysis_summary,
                    }
                )
            else:
                return (
                    jsonify(
                        {
                            "success": False,
                            "error": f"æ–‡ä»¶å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                        }
                    ),
                    500,
                )

        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ .pcapã€.pcapngæˆ–.csvæ–‡ä»¶",
                    }
                ),
                400,
            )

    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤„ç†å¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}), 500


@app.route("/api/ai_analysis", methods=["POST"])
def api_ai_analysis():
    """AIæ™ºèƒ½åˆ†æAPI - é›†æˆå®Œæ•´çš„AIæ£€æµ‹åŠŸèƒ½"""
    print(f"ğŸ¤– AIåˆ†æè¯·æ±‚å¼€å§‹ - æ—¶é—´: {datetime.now()}")

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if "file" not in request.files:
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400

        file = request.files["file"]
        if file.filename == "":
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400

        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_filename = f"{timestamp}_{filename}"
            upload_path = os.path.join(handler.upload_dir, unique_filename)

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(handler.upload_dir, exist_ok=True)
            os.makedirs(handler.output_dir, exist_ok=True)

            # ä¿å­˜æ–‡ä»¶
            file.save(upload_path)
            file_size = os.path.getsize(upload_path)
            print(f"âœ… æ–‡ä»¶å·²ä¿å­˜: {upload_path}")

            # ç”Ÿæˆä»»åŠ¡IDå¹¶ä¿å­˜åˆå§‹è®°å½•
            task_id = f"ai_task_{timestamp}"
            save_analysis_record(task_id, unique_filename, file_size, status="pending")

            # æ‰§è¡ŒçœŸå®çš„AIåˆ†æ
            try:
                # å¯¼å…¥AIæ£€æµ‹æ¨¡å—
                from complete_ai_detection import run_complete_analysis

                print("ğŸ§  å¼€å§‹çœŸå®AIåˆ†æ...")

                # å¦‚æœæ˜¯PCAPæ–‡ä»¶ï¼Œå…ˆè½¬æ¢ä¸ºCSV
                if filename.lower().endswith((".pcap", ".pcapng")):
                    try:
                        # ä½¿ç”¨PCAPå¤„ç†å™¨è½¬æ¢
                        pcap_result = handler.handle_uploaded_pcap(
                            upload_path, unique_filename
                        )
                        if pcap_result["success"] and "csv_file" in pcap_result:
                            csv_path = pcap_result["csv_file"]
                        else:
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®
                            csv_path = create_test_csv_file(
                                handler.output_dir, unique_filename
                            )
                    except Exception as e:
                        print(f"âš ï¸ PCAPè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®: {e}")
                        csv_path = create_test_csv_file(
                            handler.output_dir, unique_filename
                        )
                else:
                    # ç›´æ¥ä½¿ç”¨CSVæ–‡ä»¶
                    csv_path = upload_path

                # è¿è¡Œå®Œæ•´AIåˆ†æ
                analysis_results = run_complete_analysis(csv_path, handler.output_dir)

                if analysis_results:
                    print("âœ… AIåˆ†æå®Œæˆ")

                    # è½¬æ¢NumPyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                    def convert_numpy_types(obj):
                        """é€’å½’è½¬æ¢NumPyç±»å‹ä¸ºJSONå¯åºåˆ—åŒ–çš„Pythonç±»å‹"""
                        import numpy as np

                        if isinstance(obj, np.integer):
                            return int(obj)
                        elif isinstance(obj, np.floating):
                            return float(obj)
                        elif isinstance(obj, np.ndarray):
                            return obj.tolist()
                        elif isinstance(obj, dict):
                            return {
                                key: convert_numpy_types(value)
                                for key, value in obj.items()
                            }
                        elif isinstance(obj, list):
                            return [convert_numpy_types(item) for item in obj]
                        elif isinstance(obj, tuple):
                            return tuple(convert_numpy_types(item) for item in obj)
                        else:
                            return obj

                    # è½¬æ¢æ•´ä¸ªåˆ†æç»“æœ
                    json_safe_results = convert_numpy_types(analysis_results)

                    # æ„å»ºç»“æ„åŒ–å“åº”
                    response_data = {
                        "task_id": task_id,
                        "filename": unique_filename,
                        "basic_info": {
                            "total_flows": convert_numpy_types(
                                json_safe_results.get("basic_info", {}).get(
                                    "total_flows", 0
                                )
                            ),
                            "processing_time": convert_numpy_types(
                                json_safe_results.get("basic_info", {}).get(
                                    "processing_time", 0
                                )
                            ),
                            "features": convert_numpy_types(
                                json_safe_results.get("basic_info", {}).get(
                                    "features", 0
                                )
                            ),
                            "timestamp": json_safe_results.get("basic_info", {}).get(
                                "timestamp", datetime.now().isoformat()
                            ),
                        },
                        "protocol_analysis": convert_numpy_types(
                            json_safe_results.get("protocol_analysis", {})
                        ),
                        "service_analysis": convert_numpy_types(
                            json_safe_results.get("service_analysis", {})
                        ),
                        "state_analysis": convert_numpy_types(
                            json_safe_results.get("state_analysis", {})
                        ),
                        "pattern_analysis": convert_numpy_types(
                            json_safe_results.get("pattern_analysis", {})
                        ),
                        "attack_analysis": convert_numpy_types(
                            json_safe_results.get("attack_analysis", {})
                        ),
                        "performance_stats": convert_numpy_types(
                            json_safe_results.get("performance_stats", {})
                        ),
                        "detection_results": convert_numpy_types(
                            json_safe_results.get("detection_results", {})
                        ),
                        "enhanced_classification": convert_numpy_types(
                            json_safe_results.get("enhanced_classification", {})
                        ),
                        "accuracy_metrics": convert_numpy_types(
                            json_safe_results.get("accuracy_metrics", {})
                        ),
                        "generated_files": [
                            {
                                "name": "protocol_service_distribution.png",
                                "type": "png",
                            },
                            {"name": "traffic_patterns.png", "type": "png"},
                            {"name": "security_analysis.png", "type": "png"},
                            {"name": "performance_analysis.png", "type": "png"},
                        ],
                    }

                    # ä¿å­˜å®Œæˆçš„åˆ†æç»“æœ
                    save_analysis_record(
                        task_id,
                        unique_filename,
                        file_size,
                        response_data,
                        status="completed",
                    )

                    return jsonify(
                        {
                            "success": True,
                            "message": "AIåˆ†æå®Œæˆ",
                            "result": response_data,
                        }
                    )

                else:
                    # ä¿å­˜å¤±è´¥è®°å½•
                    save_analysis_record(
                        task_id, unique_filename, file_size, status="failed"
                    )
                    return (
                        jsonify(
                            {"success": False, "error": "AIåˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼"}
                        ),
                        500,
                    )

            except ImportError as e:
                print(f"âš ï¸ AIæ£€æµ‹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                # ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æç»“æœ
                mock_results = generate_mock_analysis_results(unique_filename)
                mock_results["task_id"] = task_id

                # ä¿å­˜æ¨¡æ‹Ÿç»“æœ
                save_analysis_record(
                    task_id,
                    unique_filename,
                    file_size,
                    mock_results,
                    status="completed",
                )

                return jsonify(
                    {
                        "success": True,
                        "message": "åˆ†æå®Œæˆ (æ¨¡æ‹Ÿæ¨¡å¼)",
                        "result": mock_results,
                    }
                )

            except Exception as e:
                print(f"âŒ AIåˆ†æå¼‚å¸¸: {e}")
                # ä¿å­˜å¤±è´¥è®°å½•
                save_analysis_record(
                    task_id, unique_filename, file_size, status="failed"
                )
                return (
                    jsonify({"success": False, "error": f"AIåˆ†æå¤±è´¥: {str(e)}"}),
                    500,
                )

        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ .pcapã€.pcapngæˆ–.csvæ–‡ä»¶",
                    }
                ),
                400,
            )

    except Exception as e:
        print(f"âŒ APIå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": f"å¤„ç†å¤±è´¥: {str(e)}"}), 500


def create_test_csv_file(output_dir, filename_base):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„CSVæ–‡ä»¶"""
    csv_filename = filename_base.replace(".pcap", ".csv").replace(".pcapng", ".csv")
    csv_path = os.path.join(output_dir, csv_filename)

    # åˆ›å»ºåŒ…å«çœŸå®æµé‡ç‰¹å¾çš„æµ‹è¯•CSV
    csv_content = """dur,proto,service,state,spkts,dpkts,sbytes,dbytes,rate,sload,dload,sloss,dloss,sinpkt,dinpkt,sjit,djit,swin,stcpb,dtcpb,dwin,tcprtt,synack,ackdat,smean,dmean,trans_depth,response_body_len,ct_srv_src,ct_state_ttl,ct_dst_ltm,ct_src_dport_ltm,ct_dst_sport_ltm,ct_dst_src_ltm,is_ftp_login,ct_ftp_cmd,ct_flw_http_mthd,ct_src_ltm,ct_srv_dst,label,attack_cat
1.5,6,http,CON,10,8,1200,800,6.67,800,533.33,0,0,120,100,0.1,0.05,8192,0,0,8192,0.1,0.05,0.02,120,100,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
0.8,6,http,FIN,5,5,600,600,6.25,750,750,0,0,120,120,0.05,0.05,4096,0,0,4096,0.08,0.04,0.02,120,120,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
2.1,6,ssh,CON,15,12,2400,1600,7.14,1142.86,761.90,0,0,160,133,0.08,0.06,8192,0,0,8192,0.12,0.06,0.03,160,133,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
0.3,17,dns,FIN,2,2,150,200,6.67,500,666.67,0,0,75,100,0.02,0.03,1024,0,0,1024,0.05,0.02,0.01,75,100,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
3.2,6,https,CON,25,20,3000,2500,7.81,937.5,781.25,0,0,120,125,0.12,0.08,8192,0,0,8192,0.15,0.08,0.04,120,125,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
0.5,6,ftp,FIN,8,6,800,600,16.0,1600,1200,0,0,100,100,0.06,0.05,4096,0,0,4096,0.08,0.04,0.02,100,100,1,0,1,1,1,1,2,0,1,1,0,1,1,0,Normal
1.8,6,smtp,CON,12,10,1500,1000,6.67,833.33,555.56,0,0,125,100,0.09,0.07,8192,0,0,8192,0.10,0.05,0.025,125,100,1,0,1,1,1,0,0,0,1,1,0,1,1,0,Normal
0.2,17,dns,FIN,1,1,80,100,5.0,400,500,0,0,80,100,0.01,0.01,512,0,0,512,0.02,0.01,0.005,80,100,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
5.0,6,http,INT,50,45,8000,7200,10.0,1600,1440,1,2,160,160,0.2,0.18,8192,0,0,8192,0.25,0.12,0.06,160,160,2,500,2,2,2,0,0,1,0,0,1,2,2,1,Reconnaissance
4.5,6,ssh,RST,40,30,6000,4500,8.89,1333.33,1000,2,1,150,150,0.25,0.20,4096,0,0,4096,0.30,0.15,0.075,150,150,1,0,3,3,3,0,0,0,0,0,0,3,3,1,Backdoor
"""

    with open(csv_path, "w", encoding="utf-8") as f:
        f.write(csv_content)

    print(f"âœ… åˆ›å»ºæµ‹è¯•CSVæ–‡ä»¶: {csv_path}")
    return csv_path


def generate_mock_analysis_results(filename):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„AIåˆ†æç»“æœ"""
    import time

    processing_time = round(2.5 + (time.time() % 3), 2)  # 2.5-5.5ç§’çš„æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

    return {
        "task_id": f"mock_task_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "filename": filename,
        "basic_info": {
            "total_flows": 1000,
            "processing_time": processing_time,
            "total_processing_time": processing_time,  # ç¡®ä¿å…¼å®¹æ€§
            "features": 42,
            "timestamp": datetime.now().isoformat(),
        },
        "protocol_analysis": {"TCP": 750, "UDP": 200, "ICMP": 50},
        "service_analysis": {
            "HTTP": 400,
            "HTTPS": 300,
            "SSH": 150,
            "DNS": 100,
            "FTP": 50,
        },
        "attack_analysis": {
            "label_distribution": {
                "Normal": 900,
                "Attack": 100,
                "Attack_Percentage": 10.0,
            },
            "suspicious_patterns": {
                "port_scan_like": 5,
                "ddos_like": 3,
                "brute_force_like": 2,
            },
        },
        "detection_results": {
            "anomaly_detection": {
                "anomalies_detected": 50,
                "anomaly_percentage": 5.0,
                "status": "completed",
            }
        },
    }


@app.route("/api/export/threats/<task_id>")
def api_export_threats(task_id):
    """å¯¼å‡ºå¨èƒæµé‡CSVæ–‡ä»¶"""
    try:
        # è·å–åˆ†æè¯¦æƒ…
        analysis_detail = get_analysis_detail(task_id)
        if not analysis_detail:
            return jsonify({"success": False, "error": "ä»»åŠ¡æœªæ‰¾åˆ°"}), 404

        analysis_results = analysis_detail.get("analysis_results", {})
        if not analysis_results:
            return jsonify({"success": False, "error": "æ²¡æœ‰åˆ†ææ•°æ®"}), 404

        # æ£€æŸ¥å¢å¼ºåˆ†ç±»æ•°æ®
        enhanced_classification = analysis_results.get("enhanced_classification", {})

        # å¦‚æœæœ‰å¯¼å‡ºæ•°æ®ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
        export_data = enhanced_classification.get("export_data", {})
        malicious_flows = export_data.get("malicious_flows", [])
        suspicious_flows = export_data.get("suspicious_flows", [])

        # å¦‚æœæ²¡æœ‰å…·ä½“çš„å¯¼å‡ºæ•°æ®ï¼Œä»åˆ†ç±»ç»Ÿè®¡ç”Ÿæˆ
        if not malicious_flows and not suspicious_flows:
            print("ğŸ“‹ ä»åˆ†ç±»ç»Ÿè®¡ç”Ÿæˆå¨èƒæµé‡æ•°æ®")
            generated_data = generate_threat_data_from_classification(
                enhanced_classification, analysis_results
            )
            malicious_flows = generated_data["malicious_flows"]
            suspicious_flows = generated_data["suspicious_flows"]

        if not malicious_flows and not suspicious_flows:
            return jsonify({"success": False, "error": "æ²¡æœ‰å¨èƒæµé‡æ•°æ®å¯å¯¼å‡º"}), 400

        print(
            f"ğŸ“Š å‡†å¤‡å¯¼å‡º: æ¶æ„æµé‡ {len(malicious_flows)} æ¡, å¯ç–‘æµé‡ {len(suspicious_flows)} æ¡"
        )

        # ç”ŸæˆCSVå†…å®¹
        csv_content = generate_threat_csv_content(malicious_flows, suspicious_flows)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"threat_traffic_{task_id}_{timestamp}.csv"

        # è¿”å›æ–‡ä»¶
        from flask import make_response

        response = make_response(csv_content)
        response.headers["Content-Disposition"] = f"attachment; filename={filename}"
        response.headers["Content-Type"] = "text/csv; charset=utf-8"

        return response

    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¨èƒæµé‡å¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"success": False, "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"}), 500


def generate_threat_data_from_classification(enhanced_classification, analysis_results):
    """ä»åˆ†ç±»ç»Ÿè®¡ç”Ÿæˆå¨èƒæµé‡æ•°æ®"""
    malicious_flows = []
    suspicious_flows = []

    try:
        # è·å–åˆ†ç±»ç»Ÿè®¡
        summary = enhanced_classification.get("classification_summary", {})
        malicious_count = summary.get("malicious_flows", 0)
        suspicious_count = summary.get("suspicious_flows", 0)

        print(f"ğŸ“ˆ åˆ†ç±»ç»Ÿè®¡: æ¶æ„æµé‡ {malicious_count}, å¯ç–‘æµé‡ {suspicious_count}")

        # è·å–æ¶æ„æµé‡è¯¦æƒ…
        malicious_details = enhanced_classification.get("malicious_traffic_details", {})
        attack_types = malicious_details.get("attack_types", {})

        # ç”Ÿæˆæ¶æ„æµé‡æ•°æ®
        flow_id = 1
        for attack_type, details in attack_types.items():
            count = details.get("count", 0)
            avg_duration = details.get("avg_duration", 1.0)
            avg_bytes = details.get("avg_bytes", 1000)

            for i in range(count):
                malicious_flows.append(
                    {
                        "flow_id": f"malicious_flow_{flow_id}",
                        "timestamp": datetime.now().isoformat(),
                        "protocol": "TCP",
                        "service": "unknown",
                        "duration": round(avg_duration + (i * 0.1), 3),
                        "src_bytes": int(avg_bytes * (0.8 + i * 0.05)),
                        "dst_bytes": int(avg_bytes * (0.6 + i * 0.03)),
                        "src_packets": 5 + i,
                        "dst_packets": 3 + i,
                        "attack_type": attack_type,
                        "threat_level": (
                            "High" if attack_type in ["DoS", "Backdoor"] else "Medium"
                        ),
                        "label": "Attack",
                    }
                )
                flow_id += 1

        # ç”Ÿæˆå¯ç–‘æµé‡æ•°æ®ï¼ˆåŸºäºå¼‚å¸¸æ£€æµ‹ï¼‰
        anomaly_detection = analysis_results.get("detection_results", {}).get(
            "anomaly_detection", {}
        )
        anomalies_detected = anomaly_detection.get("anomalies_detected", 0)

        # å–è¾ƒå°å€¼ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        actual_suspicious_count = min(suspicious_count, anomalies_detected)

        for i in range(actual_suspicious_count):
            suspicious_flows.append(
                {
                    "flow_id": f"suspicious_flow_{flow_id}",
                    "timestamp": datetime.now().isoformat(),
                    "protocol": "TCP" if i % 2 == 0 else "UDP",
                    "service": "http" if i % 3 == 0 else "unknown",
                    "duration": round(1.0 + i * 0.5, 3),
                    "src_bytes": 500 + i * 100,
                    "dst_bytes": 300 + i * 50,
                    "src_packets": 3 + i,
                    "dst_packets": 2 + i,
                    "anomaly_score": round(3.0 + i * 0.5, 2),
                    "suspicion_reason": "Statistical Anomaly",
                    "threat_level": "Medium",
                    "label": "Normal",
                }
            )
            flow_id += 1

        print(
            f"âœ… ç”Ÿæˆå¨èƒæ•°æ®: æ¶æ„ {len(malicious_flows)}, å¯ç–‘ {len(suspicious_flows)}"
        )

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå¨èƒæ•°æ®æ—¶å‡ºé”™: {e}")

    return {"malicious_flows": malicious_flows, "suspicious_flows": suspicious_flows}


def generate_threat_csv_content(malicious_flows, suspicious_flows):
    """ç”Ÿæˆå¨èƒæµé‡CSVå†…å®¹"""
    lines = []

    # CSVå¤´éƒ¨
    lines.append("# AIç™½åå•æµé‡è¿‡æ»¤ç³»ç»Ÿ - å¨èƒæµé‡å¯¼å‡º")
    lines.append(f"# å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"# æ¶æ„æµé‡æ•°: {len(malicious_flows)}")
    lines.append(f"# å¯ç–‘æµé‡æ•°: {len(suspicious_flows)}")
    lines.append("")

    # åˆ—æ ‡é¢˜
    headers = [
        "flow_id",
        "threat_category",
        "timestamp",
        "protocol",
        "service",
        "duration",
        "src_bytes",
        "dst_bytes",
        "src_packets",
        "dst_packets",
        "attack_type",
        "threat_level",
        "anomaly_score",
        "suspicion_reason",
        "label",
    ]
    lines.append(",".join(headers))

    # æ·»åŠ æ¶æ„æµé‡æ•°æ®
    for flow in malicious_flows:
        row_data = [
            str(flow.get("flow_id", "")),
            "Malicious",
            str(flow.get("timestamp", "")),
            str(flow.get("protocol", "")),
            str(flow.get("service", "")),
            str(flow.get("duration", 0)),
            str(flow.get("src_bytes", 0)),
            str(flow.get("dst_bytes", 0)),
            str(flow.get("src_packets", 0)),
            str(flow.get("dst_packets", 0)),
            str(flow.get("attack_type", "Unknown")),
            str(flow.get("threat_level", "Medium")),
            "",  # anomaly_score (æ¶æ„æµé‡æ²¡æœ‰)
            "",  # suspicion_reason (æ¶æ„æµé‡æ²¡æœ‰)
            str(flow.get("label", "Attack")),
        ]
        lines.append(",".join(row_data))

    # æ·»åŠ å¯ç–‘æµé‡æ•°æ®
    for flow in suspicious_flows:
        row_data = [
            str(flow.get("flow_id", "")),
            "Suspicious",
            str(flow.get("timestamp", "")),
            str(flow.get("protocol", "")),
            str(flow.get("service", "")),
            str(flow.get("duration", 0)),
            str(flow.get("src_bytes", 0)),
            str(flow.get("dst_bytes", 0)),
            str(flow.get("src_packets", 0)),
            str(flow.get("dst_packets", 0)),
            "",  # attack_type (å¯ç–‘æµé‡æ²¡æœ‰)
            "Low",  # threat_level
            str(flow.get("anomaly_score", 0)),
            str(flow.get("suspicion_reason", "Statistical Anomaly")),
            str(flow.get("label", "Normal")),
        ]
        lines.append(",".join(row_data))

    return "\n".join(lines)


@app.route("/api/export/csv/<task_id>")
def api_export_csv(task_id):
    """å¯¼å‡ºç‰¹å®šä»»åŠ¡çš„CSVåˆ†ææ•°æ®"""
    try:
        detail = get_analysis_detail(task_id)
        if not detail:
            return jsonify({"success": False, "error": "ä»»åŠ¡ä¸å­˜åœ¨"}), 404

        analysis_results = detail.get("analysis_results")
        if not analysis_results:
            return jsonify({"success": False, "error": "æ²¡æœ‰åˆ†ææ•°æ®å¯å¯¼å‡º"}), 404

        # ç”ŸæˆCSVå†…å®¹
        csv_content = generate_csv_export(analysis_results, detail["filename"])

        from flask import Response

        # ç”Ÿæˆæ–‡ä»¶å
        safe_filename = secure_filename(detail["filename"])
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_filename = f"AI_Analysis_{safe_filename}_{timestamp}.csv"

        return Response(
            csv_content,
            mimetype="text/csv;charset=utf-8",
            headers={
                "Content-Disposition": f"attachment; filename={export_filename}",
                "Content-Type": "text/csv;charset=utf-8",
            },
        )

    except Exception as e:
        print(f"âŒ CSVå¯¼å‡ºå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"}), 500


@app.route("/api/export/raw_csv/<task_id>")
def api_export_raw_csv(task_id):
    """å¯¼å‡ºåŸå§‹CSVæ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    try:
        # æŸ¥æ‰¾åŸå§‹CSVæ–‡ä»¶
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ–‡ä»¶å­˜å‚¨é€»è¾‘æ¥å®ç°
        csv_files = []
        for root, dirs, files in os.walk(handler.output_dir):
            for file in files:
                if file.endswith(".csv") and task_id.replace("ai_task_", "") in file:
                    csv_files.append(os.path.join(root, file))

        if not csv_files:
            return jsonify({"success": False, "error": "æœªæ‰¾åˆ°åŸå§‹CSVæ–‡ä»¶"}), 404

        # ä½¿ç”¨æœ€æ–°çš„CSVæ–‡ä»¶
        csv_file = max(csv_files, key=os.path.getmtime)

        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        download_filename = f"Raw_Data_{task_id}_{timestamp_str}.csv"

        return send_file(
            csv_file,
            mimetype="text/csv",
            as_attachment=True,
            download_name=download_filename,
        )

    except Exception as e:
        print(f"âŒ åŸå§‹CSVå¯¼å‡ºå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"}), 500


def generate_csv_export(analysis_results, filename):
    """ç”ŸæˆCSVå¯¼å‡ºå†…å®¹"""
    csv_lines = []

    # CSVå¤´éƒ¨ä¿¡æ¯
    csv_lines.append("# AIç™½åå•æµé‡è¿‡æ»¤ç³»ç»Ÿ - åˆ†ææ•°æ®å¯¼å‡º")
    csv_lines.append(f"# åŸå§‹æ–‡ä»¶: {filename}")
    csv_lines.append(f"# å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y/%m/%d %H:%M:%S')}")

    basic_info = analysis_results.get("basic_info", {})
    total_flows = basic_info.get("total_flows", 0)
    csv_lines.append(f"# æ€»æµé‡æ•°: {total_flows}")

    processing_time = basic_info.get("total_processing_time") or basic_info.get(
        "processing_time", 0
    )
    csv_lines.append(f"# å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
    csv_lines.append("")

    # 1. åè®®åˆ†ææ•°æ®
    if analysis_results.get("protocol_analysis"):
        csv_lines.append("åè®®åˆ†æ")
        csv_lines.append("åè®®ç±»å‹,æ•°é‡,ç™¾åˆ†æ¯”")
        protocol_data = analysis_results["protocol_analysis"]
        protocol_values = protocol_data.values() if protocol_data else []
        protocol_total = sum(protocol_values) if protocol_values else 1

        for protocol, count in protocol_data.items():
            percentage = (count / protocol_total) * 100
            csv_lines.append(f"{protocol},{count},{percentage:.2f}%")
        csv_lines.append("")

    # 2. æœåŠ¡ç±»å‹åˆ†ææ•°æ®
    if analysis_results.get("service_analysis"):
        csv_lines.append("æœåŠ¡ç±»å‹åˆ†æ")
        csv_lines.append("æœåŠ¡ç±»å‹,æ•°é‡,ç™¾åˆ†æ¯”")
        service_data = analysis_results["service_analysis"]
        service_values = service_data.values() if service_data else []
        service_total = sum(service_values) if service_values else 1

        for service, count in service_data.items():
            percentage = (count / service_total) * 100
            csv_lines.append(f"{service},{count},{percentage:.2f}%")
        csv_lines.append("")

    # 3. æ”»å‡»åˆ†ææ•°æ®
    attack_analysis = analysis_results.get("attack_analysis", {})
    if attack_analysis.get("label_distribution"):
        csv_lines.append("æ”»å‡»åˆ†æ")
        csv_lines.append("æ ‡ç­¾ç±»å‹,æ•°é‡,ç™¾åˆ†æ¯”")
        label_data = attack_analysis["label_distribution"]

        for label, value in label_data.items():
            if label != "Attack_Percentage":
                if isinstance(value, (int, float)):
                    basic_info = analysis_results.get("basic_info", {})
                    total_flows = basic_info.get("total_flows", 1)
                    percentage = (value / total_flows) * 100
                    csv_lines.append(f"{label},{value},{percentage:.2f}%")
                else:
                    csv_lines.append(f"{label},{value},-")
        csv_lines.append("")

    # 4. å¼‚å¸¸æ£€æµ‹ç»“æœ
    detection_results = analysis_results.get("detection_results", {})
    if detection_results.get("anomaly_detection"):
        csv_lines.append("å¼‚å¸¸æ£€æµ‹ç»“æœ")
        csv_lines.append("æ£€æµ‹é¡¹,ç»“æœ")
        anomaly = detection_results["anomaly_detection"]

        csv_lines.append(f"æ£€æµ‹åˆ°å¼‚å¸¸æ•°é‡,{anomaly.get('anomalies_detected', 0)}")
        csv_lines.append(f"å¼‚å¸¸ç™¾åˆ†æ¯”,{anomaly.get('anomaly_percentage', 0):.2f}%")
        csv_lines.append(f"æ£€æµ‹çŠ¶æ€,{anomaly.get('status', 'æœªçŸ¥')}")
        csv_lines.append("")

    # 5. å¯ç–‘æ¨¡å¼åˆ†æ
    if attack_analysis.get("suspicious_patterns"):
        csv_lines.append("å¯ç–‘æ¨¡å¼åˆ†æ")
        csv_lines.append("æ¨¡å¼ç±»å‹,æ£€æµ‹æ•°é‡")

        for pattern, count in attack_analysis["suspicious_patterns"].items():
            pattern_name = pattern.replace("_", " ").title()
            csv_lines.append(f"{pattern_name},{count}")
        csv_lines.append("")

    # 6. æ€§èƒ½ç»Ÿè®¡æ•°æ®
    if analysis_results.get("performance_stats"):
        csv_lines.append("æ€§èƒ½ç»Ÿè®¡")
        csv_lines.append("æŒ‡æ ‡,æ•°å€¼")

        for metric, value in analysis_results["performance_stats"].items():
            metric_name = metric.replace("_", " ").title()
            if isinstance(value, float):
                csv_lines.append(f"{metric_name},{value:.4f}")
            else:
                csv_lines.append(f"{metric_name},{value}")
        csv_lines.append("")

    # 7. æµé‡å¤§å°åˆ†å¸ƒ
    pattern_analysis = analysis_results.get("pattern_analysis", {})
    if pattern_analysis:
        if pattern_analysis.get("size_distribution"):
            csv_lines.append("åŒ…å¤§å°åˆ†å¸ƒ")
            csv_lines.append("å¤§å°èŒƒå›´,æ•°é‡")
            for size, count in pattern_analysis["size_distribution"].items():
                csv_lines.append(f"{size},{count}")
            csv_lines.append("")

        if pattern_analysis.get("duration_distribution"):
            csv_lines.append("è¿æ¥æŒç»­æ—¶é—´åˆ†å¸ƒ")
            csv_lines.append("æ—¶é•¿èŒƒå›´,æ•°é‡")
            duration_dist = pattern_analysis["duration_distribution"]
            for duration, count in duration_dist.items():
                csv_lines.append(f"{duration},{count}")
            csv_lines.append("")

    # 8. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    basic_info = analysis_results.get("basic_info", {})
    if basic_info:
        csv_lines.append("åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
        csv_lines.append("é¡¹ç›®,æ•°å€¼")
        csv_lines.append(f"æ€»æµé‡æ•°,{basic_info.get('total_flows', 0)}")
        csv_lines.append(f"ç‰¹å¾æ•°é‡,{basic_info.get('features', 0)}")
        proc_time = basic_info.get("total_processing_time") or basic_info.get(
            "processing_time", 0
        )
        csv_lines.append(f"å¤„ç†æ—¶é—´(ç§’),{proc_time:.2f}")
        if basic_info.get("timestamp"):
            csv_lines.append(f"åˆ†ææ—¶é—´,{basic_info['timestamp']}")

    return "\n".join(csv_lines)

@app.route("/test_traffic.csv")
def serve_test_csv():
    """æä¾›æµ‹è¯•CSVæ–‡ä»¶"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•CSVæ–‡ä»¶
    csv_content = """dur,proto,service,state,spkts,dpkts,sbytes,dbytes,rate,sload,dload,sloss,dloss,sinpkt,dinpkt,sjit,djit,swin,stcpb,dtcpb,dwin,tcprtt,synack,ackdat,smean,dmean,trans_depth,response_body_len,ct_srv_src,ct_state_ttl,ct_dst_ltm,ct_src_dport_ltm,ct_dst_sport_ltm,ct_dst_src_ltm,is_ftp_login,ct_ftp_cmd,ct_flw_http_mthd,ct_src_ltm,ct_srv_dst,label,attack_cat
1.5,6,http,CON,10,8,1200,800,6.67,800,533.33,0,0,120,100,0.1,0.05,8192,0,0,8192,0.1,0.05,0.02,120,100,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
0.8,6,http,FIN,5,5,600,600,6.25,750,750,0,0,120,120,0.05,0.05,4096,0,0,4096,0.08,0.04,0.02,120,120,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal
2.1,6,ssh,CON,15,12,2400,1600,7.14,1142.86,761.90,0,0,160,133,0.08,0.06,8192,0,0,8192,0.12,0.06,0.03,160,133,1,0,1,1,1,0,0,0,0,0,0,1,1,0,Normal"""

    from flask import Response

    return Response(
        csv_content,
        mimetype="text/csv",
        headers={"Content-Disposition": "attachment; filename=test_traffic.csv"},
    )


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ç®€åŒ–ç‰ˆAIç™½åå•æµé‡è¿‡æ»¤ç³»ç»Ÿ")
    print("=" * 50)
    print(f"ğŸ“ ä¸Šä¼ ç›®å½•: {handler.upload_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {handler.output_dir}")
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:5000")
    print("=" * 50)
    print("ğŸ”§ å¯ç”¨çš„æµ‹è¯•åŠŸèƒ½:")
    print("  - å¼€å§‹åˆ†ææŒ‰é’®")
    print("  - æµ‹è¯•æŒ‰é’®")
    print("  - è‡ªåŠ¨æµ‹è¯•ä¸Šä¼ æŒ‰é’®")
    print("  - å†å²è®°å½•æŸ¥è¯¢")
    print("=" * 50)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("templates", exist_ok=True)
    os.makedirs("static", exist_ok=True)
    os.makedirs("uploads", exist_ok=True)
    os.makedirs("processed", exist_ok=True)

    # åˆå§‹åŒ–æ•°æ®åº“
    print("ğŸ—„ï¸ åˆå§‹åŒ–æ•°æ®åº“...")
    init_database()
    print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    # ç”Ÿäº§ç¯å¢ƒé…ç½® - å…è®¸å¤–éƒ¨è®¿é—®
    app.run(host="0.0.0.0", port=5000, debug=False)

