#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´AIæ£€æµ‹æ¨¡å— - é›†æˆLSTMæ¨¡å‹æ£€æµ‹ã€æµé‡åˆ†æå’Œæ€§èƒ½è¯„ä¼°
æä¾›complete_ai_detectionçš„æ ¸å¿ƒåŠŸèƒ½
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import os
import json
from typing import Dict, List, Tuple, Any
import psutil
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    confusion_matrix,
)
from sklearn.decomposition import PCA

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from model.lstm_detector import LSTMDetector
except ImportError:
    print("âš ï¸ LSTMDetectoræ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    LSTMDetector = None

try:
    from data.unsw_nb15_preprocess import load_data_for_inference
except ImportError:
    print("âš ï¸ æ•°æ®é¢„å¤„ç†æ¨¡å—ä¸å¯ç”¨")

try:
    from enhanced_traffic_analysis import enhanced_traffic_classification
except ImportError:
    print("âš ï¸ å¢å¼ºæµé‡åˆ†ææ¨¡å—ä¸å¯ç”¨")

try:
    from ultra_clear_visualization import create_comprehensive_visualizations
except ImportError:
    print("âš ï¸ å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨")


def calculate_ai_accuracy_metrics(
    df: pd.DataFrame, ai_predictions: np.ndarray
) -> Dict[str, Any]:
    """
    è®¡ç®—AIç™½åå•æµé‡è¿‡æ»¤çš„å‡†ç¡®ç‡æŒ‡æ ‡

    Args:
        df: åŸå§‹æ•°æ®DataFrameï¼ˆåŒ…å«çœŸå®æ ‡ç­¾ï¼‰
        ai_predictions: AIæ¨¡å‹çš„é¢„æµ‹ç»“æœ

    Returns:
        åŒ…å«å„ç§å‡†ç¡®ç‡æŒ‡æ ‡çš„å­—å…¸
    """
    try:
        # ç¡®ä¿ai_predictionsæ˜¯numpyæ•°ç»„
        if isinstance(ai_predictions, list):
            ai_predictions = np.array(ai_predictions)

        # æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®æ ‡ç­¾
        if "label" not in df.columns:
            return {
                "has_ground_truth": False,
                "message": "æ•°æ®é›†æ— çœŸå®æ ‡ç­¾ï¼Œæ— æ³•è®¡ç®—å‡†ç¡®ç‡",
                "estimated_accuracy": _estimate_accuracy_by_patterns(
                    df, ai_predictions
                ),
            }

        # å¤„ç†çœŸå®æ ‡ç­¾
        true_labels = df["label"].copy()

        # æ ‡å‡†åŒ–æ ‡ç­¾æ ¼å¼
        true_binary = (true_labels == "Normal").astype(int)  # 1ä¸ºæ­£å¸¸æµé‡ï¼Œ0ä¸ºæ”»å‡»æµé‡
        pred_binary = (ai_predictions > 0.5).astype(int)  # AIé¢„æµ‹ï¼š1ä¸ºæ­£å¸¸ï¼Œ0ä¸ºæ”»å‡»

        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        from sklearn.metrics import (
            accuracy_score,
            precision_score,
            recall_score,
            f1_score,
            confusion_matrix,
        )

        accuracy = accuracy_score(true_binary, pred_binary)
        precision = precision_score(true_binary, pred_binary, zero_division=0)
        recall = recall_score(true_binary, pred_binary, zero_division=0)
        f1 = f1_score(true_binary, pred_binary, zero_division=0)

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_binary, pred_binary)
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)

        # è®¡ç®—ç™½åå•è¿‡æ»¤ç‰¹å®šæŒ‡æ ‡
        total_normal = np.sum(true_binary == 1)  # çœŸå®æ­£å¸¸æµé‡æ•°
        total_attack = np.sum(true_binary == 0)  # çœŸå®æ”»å‡»æµé‡æ•°

        # ç™½åå•å‡†ç¡®ç‡ï¼šæ­£ç¡®è¯†åˆ«ä¸ºæ­£å¸¸çš„æ­£å¸¸æµé‡ / æ€»æ­£å¸¸æµé‡
        whitelist_accuracy = tp / total_normal if total_normal > 0 else 0

        # è¯¯æŠ¥ç‡ï¼šé”™è¯¯è¯†åˆ«ä¸ºæ­£å¸¸çš„æ”»å‡»æµé‡ / æ€»æ”»å‡»æµé‡
        false_positive_rate = fp / total_attack if total_attack > 0 else 0

        # æ¼æŠ¥ç‡ï¼šé”™è¯¯è¯†åˆ«ä¸ºæ”»å‡»çš„æ­£å¸¸æµé‡ / æ€»æ­£å¸¸æµé‡
        false_negative_rate = fn / total_normal if total_normal > 0 else 0

        return {
            "has_ground_truth": True,
            "overall_accuracy": round(accuracy * 100, 2),
            "precision": round(precision * 100, 2),
            "recall": round(recall * 100, 2),
            "f1_score": round(f1 * 100, 2),
            "whitelist_accuracy": round(whitelist_accuracy * 100, 2),
            "false_positive_rate": round(false_positive_rate * 100, 2),
            "false_negative_rate": round(false_negative_rate * 100, 2),
            "confusion_matrix": {
                "true_negative": int(tn),  # æ­£ç¡®è¯†åˆ«çš„æ”»å‡»æµé‡
                "false_positive": int(fp),  # è¯¯è¯†åˆ«ä¸ºæ­£å¸¸çš„æ”»å‡»æµé‡
                "false_negative": int(fn),  # è¯¯è¯†åˆ«ä¸ºæ”»å‡»çš„æ­£å¸¸æµé‡
                "true_positive": int(tp),  # æ­£ç¡®è¯†åˆ«çš„æ­£å¸¸æµé‡
            },
            "total_samples": len(df),
            "normal_samples": int(total_normal),
            "attack_samples": int(total_attack),
        }

    except Exception as e:
        print(f"å‡†ç¡®ç‡è®¡ç®—å¤±è´¥: {e}")
        return {
            "has_ground_truth": False,
            "error": str(e),
            "estimated_accuracy": _estimate_accuracy_by_patterns(df, ai_predictions),
        }


def _estimate_accuracy_by_patterns(
    df: pd.DataFrame, ai_predictions: np.ndarray
) -> Dict[str, Any]:
    """
    å½“æ²¡æœ‰çœŸå®æ ‡ç­¾æ—¶ï¼ŒåŸºäºæµé‡æ¨¡å¼ä¼°ç®—å‡†ç¡®ç‡
    """
    try:
        print(
            f"ğŸ” å¼€å§‹ä¼°ç®—å‡†ç¡®ç‡ï¼Œæ•°æ®é•¿åº¦: {len(df)}, é¢„æµ‹é•¿åº¦: {len(ai_predictions)}"
        )

        # ç¡®ä¿ai_predictionsæ˜¯numpyæ•°ç»„
        if isinstance(ai_predictions, list):
            ai_predictions = np.array(ai_predictions)

        pred_binary = (ai_predictions > 0.5).astype(int)
        normal_ratio = np.mean(pred_binary)
        print(f"ğŸ” é¢„æµ‹çš„æ­£å¸¸æµé‡æ¯”ä¾‹: {normal_ratio}")

        # åŸºäºç½‘ç»œæµé‡çš„ç»éªŒè§„å¾‹ä¼°ç®—
        expected_normal_ratio = 0.85  # ä¸€èˆ¬ç½‘ç»œä¸­æ­£å¸¸æµé‡å 85%å·¦å³
        confidence = max(0, 100 - abs(normal_ratio - expected_normal_ratio) * 200)
        print(f"ğŸ” ç½®ä¿¡åº¦è®¡ç®—: {confidence}")

        result = {
            "estimated_normal_ratio": round(normal_ratio * 100, 2),
            "expected_normal_ratio": round(expected_normal_ratio * 100, 2),
            "confidence_score": round(confidence, 2),
            "message": "åŸºäºæµé‡æ¨¡å¼çš„ä¼°ç®—å‡†ç¡®ç‡",
        }
        print(f"ğŸ” ä¼°ç®—ç»“æœ: {result}")
        return result
    except Exception as e:
        print(f"âŒ ä¼°ç®—å‡†ç¡®ç‡å¤±è´¥: {e}")
        return {"confidence_score": 0, "message": "æ— æ³•ä¼°ç®—å‡†ç¡®ç‡"}


def _analyze_traffic_size_distribution(df: pd.DataFrame) -> Dict[str, int]:
    """åˆ†ææµé‡å¤§å°åˆ†å¸ƒ"""
    try:
        if "sbytes" not in df.columns:
            return {"æ— æ•°æ®": 0}

        sizes = df["sbytes"].fillna(0)

        # å®šä¹‰å¤§å°åŒºé—´
        bins = [0, 64, 512, 1024, 4096, float("inf")]
        labels = [
            "å°åŒ…(<64B)",
            "ä¸­å°åŒ…(64-512B)",
            "ä¸­åŒ…(512B-1KB)",
            "ä¸­å¤§åŒ…(1-4KB)",
            "å¤§åŒ…(>4KB)",
        ]

        # åˆ†ç±»ç»Ÿè®¡
        size_categories = pd.cut(sizes, bins=bins, labels=labels, include_lowest=True)
        distribution = size_categories.value_counts().to_dict()

        # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½å­˜åœ¨
        result = {}
        for label in labels:
            result[label] = int(distribution.get(label, 0))

        return result

    except Exception as e:
        print(f"æµé‡å¤§å°åˆ†æå¤±è´¥: {e}")
        return {"åˆ†æå¤±è´¥": 0}


def _analyze_duration_distribution(df: pd.DataFrame) -> Dict[str, int]:
    """åˆ†æè¿æ¥æŒç»­æ—¶é—´åˆ†å¸ƒ"""
    try:
        if "dur" not in df.columns:
            return {"æ— æ•°æ®": 0}

        durations = df["dur"].fillna(0)

        # å®šä¹‰æ—¶é—´åŒºé—´ï¼ˆç§’ï¼‰
        bins = [0, 1, 10, 60, 300, float("inf")]
        labels = [
            "ç¬æ—¶(<1s)",
            "çŸ­æ—¶(1-10s)",
            "ä¸­ç­‰(10s-1min)",
            "é•¿æ—¶(1-5min)",
            "æŒä¹…(>5min)",
        ]

        # åˆ†ç±»ç»Ÿè®¡
        duration_categories = pd.cut(
            durations, bins=bins, labels=labels, include_lowest=True
        )
        distribution = duration_categories.value_counts().to_dict()

        # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½å­˜åœ¨
        result = {}
        for label in labels:
            result[label] = int(distribution.get(label, 0))

        return result

    except Exception as e:
        print(f"è¿æ¥æ—¶é•¿åˆ†æå¤±è´¥: {e}")
        return {"åˆ†æå¤±è´¥": 0}


def _analyze_packet_count_distribution(df: pd.DataFrame) -> Dict[str, int]:
    """åˆ†æåŒ…æ•°é‡åˆ†å¸ƒ"""
    try:
        if "spkts" not in df.columns:
            return {"æ— æ•°æ®": 0}

        packet_counts = df["spkts"].fillna(0)

        # å®šä¹‰åŒ…æ•°é‡åŒºé—´
        bins = [0, 10, 50, 100, 500, float("inf")]
        labels = [
            "å°‘é‡(<10åŒ…)",
            "ä¸­å°‘(10-50åŒ…)",
            "ä¸­ç­‰(50-100åŒ…)",
            "è¾ƒå¤š(100-500åŒ…)",
            "å¤§é‡(>500åŒ…)",
        ]

        # åˆ†ç±»ç»Ÿè®¡
        packet_categories = pd.cut(
            packet_counts, bins=bins, labels=labels, include_lowest=True
        )
        distribution = packet_categories.value_counts().to_dict()

        # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½å­˜åœ¨
        result = {}
        for label in labels:
            result[label] = int(distribution.get(label, 0))

        return result

    except Exception as e:
        print(f"åŒ…æ•°é‡åˆ†æå¤±è´¥: {e}")
        return {"åˆ†æå¤±è´¥": 0}


# ç®€åŒ–çš„LSTMæ£€æµ‹å™¨ï¼ˆå¦‚æœåŸå§‹æ¨¡å—ä¸å¯ç”¨ï¼‰
class SimpleLSTMDetector(nn.Module):
    """ç®€åŒ–çš„LSTMæ£€æµ‹å™¨"""

    def __init__(self, input_dim, hidden_dim=128, num_layers=2, num_classes=2):
        super(SimpleLSTMDetector, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(
            input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2
        )
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes),
        )

    def forward(self, x):
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œæ‰©å±•ä¸º3D
        if len(x.shape) == 2:
            x = x.unsqueeze(1)

        lstm_out, (hidden, _) = self.lstm(x)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.classifier(lstm_out[:, -1, :])
        return output


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.start_memory = None
        self.start_cpu = None

    def start_detection(self):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        self.start_time = time.time()
        self.start_memory = psutil.virtual_memory().used / (1024 * 1024)  # MB
        self.start_cpu = psutil.cpu_percent(interval=None)

    def end_detection(self):
        """ç»“æŸæ€§èƒ½ç›‘æ§"""
        self.end_time = time.time()

    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.start_time is None or self.end_time is None:
            return {}

        processing_time = self.end_time - self.start_time
        end_memory = psutil.virtual_memory().used / (1024 * 1024)
        memory_usage = end_memory - self.start_memory if self.start_memory else 0
        cpu_usage = psutil.cpu_percent(interval=None)

        return {
            "processing_time": round(processing_time, 2),
            "memory_usage_mb": round(memory_usage, 2),
            "cpu_usage_percent": round(cpu_usage, 2),
            "timestamp": datetime.now().isoformat(),
        }


class CompleteAIDetector:
    """å®Œæ•´AIæ£€æµ‹å™¨ç±»"""

    def __init__(self, model_path: str = None):
        """åˆå§‹åŒ–AIæ£€æµ‹å™¨"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶
        current_dir = os.path.dirname(os.path.abspath(__file__))
        default_model_path = os.path.join(current_dir, "checkpoint_lstm1.pt")
        self.model_path = model_path or default_model_path
        self.performance_monitor = PerformanceMonitor()

    def load_model(self, input_dim: int = None):
        """åŠ è½½LSTMæ¨¡å‹"""
        try:
            # å…ˆå°è¯•åŠ è½½æ¨¡å‹æ–‡ä»¶è·å–æ­£ç¡®çš„è¾“å…¥ç»´åº¦
            if os.path.exists(self.model_path):
                checkpoint = torch.load(
                    self.model_path, map_location=self.device, weights_only=False
                )

                # ä»checkpointä¸­æ¨æ–­è¾“å…¥ç»´åº¦
                if "lstm.weight_ih_l0" in checkpoint:
                    model_input_dim = checkpoint["lstm.weight_ih_l0"].shape[1]
                    print(f"ğŸ“ ä»æ¨¡å‹æ–‡ä»¶æ¨æ–­è¾“å…¥ç»´åº¦: {model_input_dim}")
                else:
                    model_input_dim = input_dim if input_dim else 42
                    print(f"ğŸ“ ä½¿ç”¨é»˜è®¤è¾“å…¥ç»´åº¦: {model_input_dim}")
            else:
                model_input_dim = input_dim if input_dim else 42
                print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦: {model_input_dim}")

            # ä¼˜å…ˆä½¿ç”¨åŸå§‹LSTMDetectorï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            if LSTMDetector:
                self.model = LSTMDetector(model_input_dim).to(self.device)
            else:
                self.model = SimpleLSTMDetector(model_input_dim).to(self.device)

            if os.path.exists(self.model_path):
                self.model.load_state_dict(
                    torch.load(
                        self.model_path, map_location=self.device, weights_only=False
                    )
                )
                self.model.eval()
                print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
                return True
            else:
                print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                return False
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def preprocess_data(self, csv_path: str) -> Tuple[torch.Tensor, pd.DataFrame, Dict]:
        """é¢„å¤„ç†CSVæ•°æ®"""
        try:
            df = pd.read_csv(csv_path)
            print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

            # åŸºæœ¬æ•°æ®é¢„å¤„ç†
            df = self._clean_data(df)

            # ç‰¹å¾å·¥ç¨‹
            features = self._extract_features(df)

            # è½¬æ¢ä¸ºtensor
            X = torch.FloatTensor(features).to(self.device)

            basic_info = {
                "total_flows": len(df),
                "features": features.shape[1] if len(features.shape) > 1 else 1,
                "timestamp": datetime.now().isoformat(),
            }

            return X, df, basic_info

        except Exception as e:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return None, None, None

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®"""
        # å¤„ç†ç¼ºå¤±å€¼
        df = df.fillna(0)

        # å¤„ç†æ— é™å€¼
        df = df.replace([np.inf, -np.inf], 0)

        # ç¡®ä¿æ•°å€¼ç±»å‹
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].astype(float)

        return df

    def _extract_features(self, df: pd.DataFrame) -> np.ndarray:
        """æå–ç‰¹å¾å‘é‡"""
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        feature_columns = [
            "dur",
            "spkts",
            "dpkts",
            "sbytes",
            "dbytes",
            "rate",
            "sttl",
            "dttl",
            "sload",
            "dload",
            "sloss",
            "dloss",
            "sinpkt",
            "dinpkt",
            "sjit",
            "djit",
            "swin",
            "stcpb",
            "dtcpb",
            "dwin",
            "tcprtt",
            "synack",
            "ackdat",
            "smean",
            "dmean",
            "trans_depth",
            "response_body_len",
            "ct_srv_src",
            "ct_state_ttl",
            "ct_dst_ltm",
            "ct_src_dport_ltm",
            "ct_dst_sport_ltm",
            "ct_dst_src_ltm",
            "ct_ftp_cmd",
            "ct_flw_http_mthd",
            "ct_src_ltm",
            "ct_srv_dst",
            "is_ftp_login",
            "is_sm_ips_ports",
        ]

        # é€‰æ‹©å­˜åœ¨çš„ç‰¹å¾åˆ—
        available_features = [col for col in feature_columns if col in df.columns]

        if not available_features:
            # å¦‚æœæ²¡æœ‰é¢„å®šä¹‰ç‰¹å¾ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼åˆ—
            available_features = df.select_dtypes(include=[np.number]).columns.tolist()

        # æå–ç‰¹å¾çŸ©é˜µ
        features = df[available_features].values

        # æ ‡å‡†åŒ–å¤„ç†
        features = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)

        return features

    def detect_traffic(self, X: torch.Tensor) -> Dict:
        """æ‰§è¡ŒAIæ£€æµ‹"""
        if self.model is None:
            return {"error": "æ¨¡å‹æœªåŠ è½½"}

        try:
            self.performance_monitor.start_detection()

            with torch.no_grad():
                # æ‰¹é‡é¢„æµ‹
                batch_size = 256
                predictions = []
                probabilities = []

                for i in range(0, len(X), batch_size):
                    batch = X[i : i + batch_size]
                    output = self.model(batch)

                    # è·å–é¢„æµ‹å’Œæ¦‚ç‡
                    pred = output.argmax(dim=1)
                    prob = torch.softmax(output, dim=1)

                    predictions.extend(pred.cpu().numpy())
                    probabilities.extend(prob.cpu().numpy())

            predictions = np.array(predictions)
            probabilities = np.array(probabilities)

            self.performance_monitor.end_detection()

            # ç»Ÿè®¡ç»“æœ
            normal_count = np.sum(predictions == 0)
            attack_count = np.sum(predictions == 1)
            total_count = len(predictions)

            detection_results = {
                "total_flows": total_count,
                "normal_flows": int(normal_count),
                "attack_flows": int(attack_count),
                "normal_percentage": (
                    float(normal_count / total_count * 100) if total_count > 0 else 0
                ),
                "attack_percentage": (
                    float(attack_count / total_count * 100) if total_count > 0 else 0
                ),
                "predictions": predictions.tolist(),
                "probabilities": probabilities.tolist(),
                "confidence_scores": np.max(probabilities, axis=1).tolist(),
            }

            return detection_results

        except Exception as e:
            print(f"âŒ AIæ£€æµ‹å¤±è´¥: {e}")
            return {"error": str(e)}


def create_visualizations(df: pd.DataFrame, detection_results: Dict, output_dir: str):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    try:
        os.makedirs(output_dir, exist_ok=True)

        # 1. åè®®åˆ†å¸ƒå›¾
        plt.figure(figsize=(10, 6))
        if "proto" in df.columns:
            proto_counts = df["proto"].value_counts().head(10)
            plt.subplot(2, 2, 1)
            proto_counts.plot(kind="bar")
            plt.title("Protocol Distribution")
            plt.xticks(rotation=45)

        # 2. æœåŠ¡åˆ†å¸ƒå›¾
        if "service" in df.columns:
            plt.subplot(2, 2, 2)
            service_counts = df["service"].value_counts().head(10)
            service_counts.plot(kind="bar")
            plt.title("Service Distribution")
            plt.xticks(rotation=45)

        # 3. æ£€æµ‹ç»“æœé¥¼å›¾
        plt.subplot(2, 2, 3)
        labels = ["Normal", "Attack"]
        sizes = [
            detection_results.get("normal_flows", 0),
            detection_results.get("attack_flows", 0),
        ]
        colors = ["lightgreen", "lightcoral"]
        plt.pie(sizes, labels=labels, colors=colors, autopct="%1.1f%%", startangle=90)
        plt.title("Detection Results")

        # 4. æµé‡æ—¶é—´åºåˆ—ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ï¼‰
        plt.subplot(2, 2, 4)
        if "dur" in df.columns:
            plt.hist(df["dur"], bins=50, alpha=0.7, color="skyblue")
            plt.title("Duration Distribution")
            plt.xlabel("Duration (seconds)")
            plt.ylabel("Frequency")

        plt.tight_layout()
        plot_path = os.path.join(output_dir, "traffic_analysis.png")
        plt.savefig(plot_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        return plot_path

    except Exception as e:
        print(f"âŒ å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")
        return None


def analyze_attack_patterns(df: pd.DataFrame, predictions: List[int]) -> Dict:
    """åˆ†ææ”»å‡»æ¨¡å¼"""
    try:
        attack_analysis = {}

        # åˆ›å»ºé¢„æµ‹æ ‡ç­¾
        df_with_pred = df.copy()
        df_with_pred["prediction"] = predictions

        # æ”»å‡»æµé‡åˆ†æ
        attack_flows = df_with_pred[df_with_pred["prediction"] == 1]

        if len(attack_flows) > 0:
            attack_analysis = {
                "attack_count": len(attack_flows),
                "attack_protocols": (
                    attack_flows["proto"].value_counts().to_dict()
                    if "proto" in attack_flows.columns
                    else {}
                ),
                "attack_services": (
                    attack_flows["service"].value_counts().to_dict()
                    if "service" in attack_flows.columns
                    else {}
                ),
                "avg_duration": (
                    float(attack_flows["dur"].mean())
                    if "dur" in attack_flows.columns
                    else 0
                ),
                "avg_bytes": (
                    float(
                        (
                            attack_flows.get("sbytes", 0)
                            + attack_flows.get("dbytes", 0)
                        ).mean()
                    )
                    if "sbytes" in attack_flows.columns
                    else 0
                ),
            }

        return attack_analysis

    except Exception as e:
        print(f"âŒ æ”»å‡»æ¨¡å¼åˆ†æå¤±è´¥: {e}")
        return {}


def run_complete_analysis(csv_path: str, output_dir: str = ".") -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„AIåˆ†ææµç¨‹

    Args:
        csv_path: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        å®Œæ•´çš„åˆ†æç»“æœå­—å…¸
    """
    try:
        print("ğŸš€ å¼€å§‹å®Œæ•´AIåˆ†ææµç¨‹...")

        # 1. åˆå§‹åŒ–AIæ£€æµ‹å™¨
        detector = CompleteAIDetector()

        # 2. æ•°æ®é¢„å¤„ç†
        print("ğŸ“Š æ•°æ®é¢„å¤„ç†...")
        X, df, basic_info = detector.preprocess_data(csv_path)

        if X is None or df is None:
            return {"error": "æ•°æ®é¢„å¤„ç†å¤±è´¥"}

        # 3. åŠ è½½æ¨¡å‹ï¼ˆå…ˆåŠ è½½ä»¥è·å–æ­£ç¡®ç»´åº¦ï¼‰
        print("ğŸ§  åŠ è½½AIæ¨¡å‹...")
        model_loaded = detector.load_model()

        if not model_loaded:
            return {"error": "æ¨¡å‹åŠ è½½å¤±è´¥"}

        # 4. æ ¹æ®æ¨¡å‹è°ƒæ•´æ•°æ®ç»´åº¦
        expected_dim = None
        if hasattr(detector.model, "lstm"):
            expected_dim = detector.model.lstm.weight_ih_l0.shape[1]
        elif hasattr(detector.model, "classifier"):
            # å¯¹äºç®€åŒ–ç‰ˆæœ¬ï¼Œä»åˆ†ç±»å™¨æ¨æ–­
            pass

        if expected_dim and expected_dim != X.shape[1]:
            print(f"âš™ï¸ è°ƒæ•´ç‰¹å¾ç»´åº¦: {X.shape[1]} -> {expected_dim}")
            if X.shape[1] < expected_dim:
                # å¦‚æœå½“å‰ç‰¹å¾å°‘äºæœŸæœ›ï¼Œè¡¥é›¶
                padding = torch.zeros(X.shape[0], expected_dim - X.shape[1]).to(
                    X.device
                )
                X = torch.cat([X, padding], dim=1)
            elif X.shape[1] > expected_dim:
                # å¦‚æœå½“å‰ç‰¹å¾å¤šäºæœŸæœ›ï¼Œæˆªå–
                X = X[:, :expected_dim]

        # 4. æ‰§è¡ŒAIæ£€æµ‹
        print("ğŸ” æ‰§è¡ŒAIæ£€æµ‹...")
        detection_results = detector.detect_traffic(X)

        if "error" in detection_results:
            return detection_results

        # 6. è®¡ç®—AIå‡†ç¡®ç‡æŒ‡æ ‡
        print("ğŸ“Š è®¡ç®—AIå‡†ç¡®ç‡æŒ‡æ ‡...")
        accuracy_metrics = calculate_ai_accuracy_metrics(
            df, detection_results["predictions"]
        )
        print(f"âœ… å‡†ç¡®ç‡åˆ†æå®Œæˆ: {accuracy_metrics.get('has_ground_truth', False)}")
        if accuracy_metrics.get("has_ground_truth"):
            print(f"   æ€»ä½“å‡†ç¡®ç‡: {accuracy_metrics.get('overall_accuracy', 0)}%")
            print(f"   ç™½åå•å‡†ç¡®ç‡: {accuracy_metrics.get('whitelist_accuracy', 0)}%")

        # 7. æ€§èƒ½ç»Ÿè®¡
        performance_stats = detector.performance_monitor.get_stats()

        # 8. æ”»å‡»æ¨¡å¼åˆ†æ
        print("ğŸ¯ åˆ†ææ”»å‡»æ¨¡å¼...")
        attack_analysis = analyze_attack_patterns(df, detection_results["predictions"])

        # 9. å¢å¼ºæµé‡åˆ†ç±»ï¼ˆå¦‚æœæ¨¡å—å¯ç”¨ï¼‰
        enhanced_classification = {}
        try:
            print("ğŸ” æ‰§è¡Œå¢å¼ºæµé‡åˆ†ç±»åˆ†æ...")

            # è°ƒç”¨å¢å¼ºæµé‡åˆ†ç±»å‡½æ•°
            if "enhanced_traffic_classification" in globals():
                enhanced_classification = enhanced_traffic_classification(df)
                print("âœ… å¢å¼ºæµé‡åˆ†ç±»å®Œæˆ")
            else:
                print("âš ï¸ å¢å¼ºæµé‡åˆ†ç±»æ¨¡å—ä¸å¯ç”¨")
                enhanced_classification = {"status": "module_unavailable"}
        except Exception as e:
            print(f"âš ï¸ å¢å¼ºæµé‡åˆ†ç±»å¤±è´¥: {e}")
            enhanced_classification = {"error": str(e)}

        # 8. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        print("ğŸ“ˆ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        visualization_path = create_visualizations(df, detection_results, output_dir)

        # 9. åè®®å’ŒæœåŠ¡åˆ†æ
        protocol_analysis = {}
        service_analysis = {}
        state_analysis = {}

        if "proto" in df.columns:
            protocol_analysis = df["proto"].value_counts().to_dict()

        if "service" in df.columns:
            service_analysis = df["service"].value_counts().to_dict()

        if "state" in df.columns:
            state_analysis = df["state"].value_counts().to_dict()

        # 10. æ„å»ºå®Œæ•´ç»“æœ
        complete_results = {
            "basic_info": {
                **basic_info,
                "processing_time": performance_stats.get("processing_time", 0),
            },
            "detection_results": detection_results,
            "accuracy_metrics": accuracy_metrics,
            "performance_stats": performance_stats,
            "attack_analysis": attack_analysis,
            "protocol_analysis": protocol_analysis,
            "service_analysis": service_analysis,
            "state_analysis": state_analysis,
            "enhanced_classification": enhanced_classification,
            "pattern_analysis": {
                "high_risk_flows": detection_results.get("attack_flows", 0),
                "normal_flows": detection_results.get("normal_flows", 0),
                "confidence_distribution": {
                    "high": len(
                        [
                            c
                            for c in detection_results.get("confidence_scores", [])
                            if c > 0.8
                        ]
                    ),
                    "medium": len(
                        [
                            c
                            for c in detection_results.get("confidence_scores", [])
                            if 0.5 < c <= 0.8
                        ]
                    ),
                    "low": len(
                        [
                            c
                            for c in detection_results.get("confidence_scores", [])
                            if c <= 0.5
                        ]
                    ),
                },
                "size_distribution": _analyze_traffic_size_distribution(df),
                "duration_distribution": _analyze_duration_distribution(df),
                "packet_distribution": _analyze_packet_count_distribution(df),
            },
            "visualization_path": visualization_path,
            "timestamp": datetime.now().isoformat(),
            "status": "success",
        }

        # 11. ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        result_file = os.path.join(
            output_dir,
            f"ai_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        )
        try:
            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(
                    complete_results, f, indent=2, ensure_ascii=False, default=str
                )
            print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {result_file}")
        except Exception as e:
            print(f"âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {e}")

        print("âœ… å®Œæ•´AIåˆ†ææµç¨‹å®Œæˆï¼")
        return complete_results

    except Exception as e:
        print(f"âŒ å®Œæ•´AIåˆ†æå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return {"error": str(e), "timestamp": datetime.now().isoformat()}


if __name__ == "__main__":
    # æµ‹è¯•å‡½æ•°
    test_csv = "test_data.csv"
    if os.path.exists(test_csv):
        results = run_complete_analysis(test_csv)
        print(json.dumps(results, indent=2, ensure_ascii=False, default=str))
    else:
        print("æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
